\ignore{Many data and knowledge management problems require some sort of
textual inference. These range from query expansion and interpretation
in information retrieval to query schema matching and question
answering. \ignore{Especially, in advanced web search and contextual
  advertising, textual inference plays an important role as the core
  technique to finding related information that can attract users'
  interest. For instance, while searching for the reviews of {\em
    Nikon D90}, one may also be interested in reading the reviews for
  other {\em Nikon's cameras}, or {\em cameras} in general. To satisfy
  users' interest, advanced search engines need to be equipped with
  textual inference techniques that can perform search on related
  concepts in the index of web documents.}} 

Textual inference requires the use of large amounts of background
knowledge. For example, it may be important to know that a {\em blue
  Toyota} is not a {\em red Toyota} nor a {\em blue Honda} but that
all are cars, and even Japanese made cars. Word in Textual Entailment
~\cite{DaganGlMa06,HaghighiNMa05,BGPRS05}, has argued quite
convincingly, (e.g.~\cite{maccartney-manning:2008:PAPERS}), that many
inferences are largely compositional and depend on the ability of
models to recognize taxonomic relations between noun phrases and entities. For example, it is often necessary to know of an {\em ancestor} relation and its directionality in order to deduce that a statement with respect to the {\em child} (e.g., {\em cannabis}) holds for an {\em ancestor} (e.g.,{\em drugs}) as in the following example, taken from the textual entailment
challenge data set:

{\small
  \begin{quote}
    {\bf T}: Nigeria's National Drug Law Enforcement Agency (NDLEA)
    has seized 80 metric tonnes of {\em cannabis} in one of its
    largest ever hauls, officials say.

    {\bf H}: Nigeria seizes 80 tonnes of {\em drugs}.
  \end{quote}
}

Similarly, it is often important to know of a {\em sibling} relation
to infer that a statement about {\em Taiwan} may (without additional information){\em contradict} an identical statement with respect to {\em Japan}  since these are {\em different} countries, as
in the following example:

{\small
  \begin{quote}

    {\bf T}: A strong earthquake struck off the southern tip of {\em
      Taiwan} at 12:26 UTC, triggering a warning from Japan's
    Meteorological Agency that a 3.3 foot tsunami could be heading
    towards Basco, in the Philippines.

    {\bf H}: An earthquake strikes {\em Japan}.
  \end{quote}
}

A lot of work has been devoted to acquiring semantic taxonomies and
ontologies \cite{Snow2006,suchanek2007WWW} resulting in structured
knowledge sources such as augmented WordNet and Yago which represent
taxonomic information about individual concepts
(Sec.~\ref{sec:related-work}). However, as we show, these suffer from
limited coverage and, more importantly, need to represent uncertainty
with respect to each concept, making it difficult to support robust
classification of taxonomic relations between two given concepts.

%In this paper, we present a novel approach to the fundamental problem
%of recognizing taxonomic relations between two given concepts. 

Identifying and classifying taxonomic relations between two given concepts serves a different purpose and is distinct from that of Open Information Extraction \cite{BCSBE07}, On-Demand Information Extraction \cite{Sekine06} and other effort to recognize {\em easy to find} facts in a given
corpus~~\cite{davidov-rappoport:2008:ACLMain2,pacsca-vandurme:2008:ACLMain}--capitalizing on local co-occurrence of concepts to generate databases of open-ended facts.
%
It is also different from the supervised relation
extraction~\cite{RothYi04} effort which requires additional supervised
data to learn new relations.
%
%
% The key reason is that the knowledge acquisition
%methods alluded to know of {\em X} and {\em Y} only if they have
%occurred in an explicit way and in close proximity in a
%sentence. 
%Indeed, we know of no successful application of the large
%scale existential knowledge acquisition efforts to textual inference.

We believe that these acquired structured knowledge bases and ontologies are important resources in order to support robust classification of taxonomic relations but, as we show, are not sufficient. There is a need to fuse information from multiple sources, including unstructured text (e.g., the web) in order to determine the taxonomic relation between a pair of concepts.

\ignore{
This paper proposes to address the problem of on-the-fly taxonomic
relation identification and classification in a form that is directly
applicable to textual inference. In the paper of
\cite{maccartney-manning:2008:PAPERS}, it is shown that taxonomic
relations are key relations for textual inference. In their work, {\em
  sibling} relation is referred as {\em alternation}, and {\em
  ancestor} (or {\em is-a}) relation is referred as {\em forward
  entailment} and {\em backward entailment}. Following their argument,
we expect that the resource developed in this work can be used
compositionally to support robust textual inference.
}
\ignore{Specifically, our system
  accepts two input concepts as arguments (these could be entities or
  noun phrases) and identifies the relation between them \ignore{along
    with its possible label}. For example, we identify that {\em
    global warming} and {\em food crisis} are in a {\em sibling}
  relation, and the concept of {\em economic problems} is in an {\em
    ancestor} relation with both of them. We focus here on the {\em
  ancestor} (or {\em is-a}) relation and the {\em sibling} relation that were
identified as key relations also in
\cite{maccartney-manning:2008:PAPERS} (they call a {\em sibling}
relation an {\em alternation}, and our {\em ancestor} relation {\em
  forward entailment} and {\em backward entailment}). Following their
argument, we expect that the resource developed in this work can be
used compositionally to support robust textual inference.}

\ignore{ Our approach can discover whether two input entities pose an
  alternation (or non-exhaustive exclusion) relation ({\em red} $|$
  {\em green}), forward entailment ({\em Mel Gibson} $\sqsubset$ {\em
    actor}), reverse entailment ({\em flower} $\sqsupset$ {\em lily}),
  or independence ({\em Boeing 747} $\#$ {\em Valentine}). }


We present an approach that makes use of machine learning and a constrained optimization based decision algorithm to combine information from structured sources and unstructured text, and show that it significantly improves taxonomic relation identification relative to existing structured knowledge sources.

Our key technical contribution is a constraint-based framework that
makes use of relational constraints in a global inference process that accurately identifies taxonomic relations. Our inference algorithm makes use of an accurate classifier we develop, which returns, for a given pair of concepts, a distribution over possible taxonomic relations; this classifier is applied multiple times, on an automatically generated network of concepts that
are related to the target pair, and the constrained optimization
technique is used to force these decisions to cohere across the network. This results in improving the accuracy of each of the predicted relations in the network.
%
Our basic classifier makes use of Wikipedia and its category structure. On one hand, this guarantees growing coverage but, on the other, necessitates taking into account the non-uniformity and level of noise in this resource.  Our algorithmic approach therefore treats Wikipedia and its category structure as an open resource and uses statistical text mining techniques to gather robust
information. And, while Wikipedia has broad coverage, there is a need to
go beyond it.  We suggest a simple but efficient technique to
accomplish this using web search, and show its effectiveness when at
least one of the target concepts is not mentioned in Wikipedia.

%
\ignore{For example, the concept {\em Ford} appears many times in
  Wikipedia and is part of a large number of categories. As a {\em
    president}, mentions of {\em Ford} are consistent with mentions of
  other presidents. However, {\em Ford} also appears in other senses,
  for example, related to the {\em car} industry.  We need to
  disambiguate it, determine which category it belongs to and,
  within this category, which specific concept is intended. In order
  to disambiguate it, we make use of the context provided by the
  concept pair---{\em Ford} in ({\em Ford}, {\em Nixon}) is probably
  different than the one in ({\em Ford}, {\em Chevrolet}) as well as
  the one in ({\em Ford}, {\em Iacocca}).}
%
\ignore{Textual inference is driven by background
  knowledge. Therefore, the notion of {\em prominence} is
  essential. Most people know that {\em Michael Jordan} is a former
  NBA player, but they most likely do not know the {\em Michael
    Jordan} who attends school with the authors.  Consequently, unless
  additional knowledge is given, a textual inference system should
  assume that {\em Michael Jordan} is a basketball player. This is why
  we use Wikipedia as our background knowledge source. Moreover, under
  the assumption that in textual inference applications we are in
  search of some notion of ``common sense'' knowledge, we make use of
  a notion of prominence with respect to a given text collection (in
  this case, with respect to Wikipedia itself).}
  %


\ignore{Our key technical contribution is a novel approach that makes use of
constraint-based inference with relational constraints to accurately
identify relations; we make use of our machine learning approach
multiple times, on automatically generated network of concepts that
are related to the target pair, and use constrained optimization
techniques to force these decisions to be coherent, thus improving the
accuracy of the decision.}

\ignore{We measure the performance of our system over a large number
  of pairs chosen from over 40 semantic classes. We compare it with
  other large scale efforts to identify relations between
  concepts. For example we show that, even when all concepts are
  covered by the {\em extended WordNet}~\cite{Snow2006}, our system
  still significantly outperforms that system. }
%
%We also show examples indicating the contribution of our
%relation identification to textual inference.

The contributions of this paper are:

\begin{enumerate}
%\item The definition of a taxonomic relation identification problem so
%  that it is directly relevant to supporting textual inference
\item The development of a robust and accurate machine learning-based
  approach to classify taxonomic relations. 
\item A constraint-based inference model that incorporates prior
  knowledge to accurately identify concept taxonomic relations.
\item An approach to leveraging an existing knowledge base, which by
  itself is weaker than our relation identifier, to provide useful information for the inference model.
\end{enumerate}

In an extensive experimental study we show that our approach significantly improves taxonomic relation identification relative to existing structured knowledge sources.

The next section gives an overview of our algorithmic approach. In
Sec. \ref{sec:inference} we present the constraint-based inference
model that makes used of global relational constraints to infer concepts' taxonomic relations. As shown, in order to generate a network of related concepts for the inference process we leverage the \textsc{Yago} ontology. Our machine learning-based component is used to make local prediction on taxonomic
relations  and is described in Sec. \ref{sec:learning}. Sec. \ref{sec:experiments} presents our experimental evaluation, and Sec. \ref{sec:related-work} describes some of the related work. 
%We conclude the paper in Sec. \ref{sec:conclusions}.

\ignore{Notably, our algorithmic approach is trained with a small
  number of annotated examples and generalizes well across semantic
  classes.}

\ignore{The rest of this paper is organized as follows. In
  Section~\ref{sec:relatedwork}, we briefly mention about previous
  work that inspired our approach.  Section~\ref{sec:approach}
  formalizes the problem and describes our algorithmic approach to
  relation detection and classification. Our experiments and results
  are described in Section \ref{sec:experiments}. Discussion and
  future work are in Section \ref{sec:discussion}.}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "jupiter"
%%% End:
