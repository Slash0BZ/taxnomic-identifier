\ignore{From real-case observations, we discover several relational
constraints among concept relations that can be used to enforce the
relation between two input concepts identified by our relation
classifier.}

Analyzing concepts and the relations between them reveals several
relational constraints among the relations identified for the target
pair and those identified for related concepts. For instance, 
{\em George W. Bush} cannot be an ancestor or sibling of {\em
  president} if we are confident that concept {\em president} is an
ancestor of {\em Bill Clinton}, and {\em Bill Clinton} is a
sibling of {\em George W. Bush}. Another example would be that
if concepts {\em red} and {\em green} are known to be siblings, and {\em blue} is also known to be a sibling of {\em red}, the
prediction that {\em green} is an ancestor of {\em blue} should be
invalid. We call the combination of concepts and their relations {\em
  concept network}. Fig. \ref{fig:triangles} shows some $n$-node
concept networks consisting of two input concepts ($x$, $y$), and
additional concepts $z$, $w$, $v$. Fig. \ref{fig:triangles}(b) and
\ref{fig:triangles}(d) illustrate two {\em relational constraints}
which will be defined later. In general, $n$ concepts can be involved
to construct $n$-node concept networks ($n > 2$). In this paper, we
focus on $3$-node concept networks consisting of the two
target concepts and an additional one. However, our formalization
applies to general $n$-node concept networks.
% The problem of determining optimal value of $n$ is beyond the scope
% of this paper.

\ignore{From our observations, we see that if we can obtain additional
  concepts in addition to the two input concepts, we can enforce such
  relational constraints as prior knowledge to guide the final
  decision of the relation identifier. In the literature, several
  models take advantage of prior knowledge to achieve significant
  improvement in performance
  \cite{CGRT09,DenisBa07,PunyakanokRoYi05}. There are two main
  approaches to inject prior knowledge. The first approach
  incorporates prior knowledge {\em indirectly}; by adding more
  features \cite{RothYi05}. The other incorporates prior knowledge
  {\em directly} in the form of hard or soft constraints
  \cite{ChangRaRo08}. We want to incorporate prior knowledge directly,
  therefore our model is closely related to the latter approach.}

The aforementioned observations show that, if we can obtain additional
concepts that are related to the two input concepts, we can enforce
such relational constraints as prior knowledge to make final
decision. Our inference model follows constraint-based formulations that
were introduced in the NLP community and were shown to be very
effective in exploiting declarative background knowledge as a way to
achieve significant improvement in performance
\cite{RothYi04,PRYZ05,CGRT09,DenisBa07,PunyakanokRoYi05}. While it is
also possible to inject prior knowledge {\em indirectly}, by adding
more features, it has been argued (e.g., \cite{RothYi05,ChangRaRo08})
that a {\em direct} way, in the form of soft or hard constraints, is
beneficial due to better expressivity and simpler learning. Our model
follows this approach.

\begin{figure}[!t]
  \centering
  \includegraphics[totalheight=0.107\textheight]{networks}
  \caption{{\small Examples of $n$-node concept networks formed by two input
    concepts $x$, $y$, and related concepts $z$, $w$ and $v$. Concept
    networks (a) and (c) show valid combinations of edges, whereas (b)
    and (d) demonstrate invalid combinations. (b) and (d) are two
    relational constraints that we do not allow. For simplicity, we do
    not draw {\em no relation} edges in (d).}}
  \label{fig:triangles}
\end{figure}

\subsection{Constraints as Prior Knowledge}
\label{sec:cons-prior-know}
Let $x, y$ be the two input concepts let $\mathcal{Z}=\{z_1, z_2, ...,
z_m\}$ be a set of additional concepts. For a subset $Z \in
\mathcal{Z}$, we construct a set of concept networks whose nodes are
$x$, $y$ and all elements in $Z$; and the edge, $e$, between every two
nodes is one of four taxonomic relations whose weight, $w(e)$, is
given by a local classifier (see Sec. \ref{sec:learning}). If $m =
|Z|$, there will be $n=m+2$ nodes in each network, and $4^{\left [
    \frac{1}{2} n(n-1) \right ] }$ concept networks in total.
\ignore{For a subset $Z \in \mathcal{Z}$, a set of concept networks is
  constructed. Each concept network contains two input concepts $x$,
  and $y$, and all additional concepts in $Z$. Every two concepts in a
  concept network are connected by their relations which are the four
  relations of interest in this paper. A local relation classifier is
  used to give weights for 4 relation classes of an edge in a concept
  network. We use $e$ and $w(e)$ to denote an edge in a concept
  network and its corresponding weight, respectively. If $n$ is the
  number of concepts in a network ($n > 2$), there will be $4^{\left [
      \frac{1}{2} n(n-1) \right ] }$ concept networks that can be
  constructed because there are $4$ relations for every $2$ concepts.}
With $m=1$, each network consists of 3 nodes, and there are $64$
concept networks.
%

A {\em relational constraint} is defined as an invalid concept network
(regarding only its edges' setting) that is not allowed to happen
(e.g. Fig. \ref{fig:triangles}(b),
Fig. \ref{fig:triangles}(d)). \ignore{Figure \ref{fig:triangles}(b)
  demonstrates a relational constraint where the combination of edges
  is ({\em sibling}, {\em sibling}, {\em ancestor}) moving clockwise
  direction with respect to $x$ and $y$.}

Let $\mathcal{C}$ be a list of relational constraints. Following the
approaches to inject prior knowledge directly in \cite{ChangRaRo08},
we incorporate relational constraints into our model to choose the
best network $t^*$, which also gives the relation between $x$ and $y$,
from a set of concept networks constructed from $\left < x, y, Z
\right >$. The scoring function is a linear combination of the edge
weights $w(e)$ and the penalties $\rho_k$ of concept networks
violating constraint $C_k \in \mathcal{C}$.
%
\begin{equation}
  \label{eqn:tscore}
  t^* = \text{argmax}_{t} \sum_{e \in t} w(e) - \sum_{k=1}^{|\mathcal{C}|} \rho_{k} d_{C_k}(t)
\end{equation}
%
In Eqn. \ref{eqn:tscore}, function $d_{C_k}(t)$ measures the degree
that concept network $t$ violates constraint $C_k$.  In our work, we
use relational constraints as hard constraints and set their penalty
$\rho_k$ to $\infty$. Objective function \ref{eqn:tscore} allows us to
pick the best setting of all edges connecting concepts in the networks
with respect to $\mathcal{C}$.
%
After picking the best concept network $t^*$ for every $Z \in
\mathcal{Z}$, we make the final decision on the taxonomic relation
between $x$ and $y$. Let $\ell$ denote the relation (i.e. edge) between $x$
and $y$ in $t^*$. The set of all $t^*$ is divided into
groups with respect to $\ell$. We denote these groups as
$\mathcal{T}_{\ell}$. To choose the best taxonomic realtion, $\ell^*$,
of $x$ and $y$, we solve the objective function defined in
Eqn. \ref{eqn:objectivefunction}.
%
\ignore{ After collecting all the topmost concept networks by going
  through all $Z \in \mathcal{Z}$, the topmost concept networks are
  divided into four groups with respect to the relation of $x$ and $y$
  in each network. Each group of the topmost concept networks is
  denoted by $\mathcal{T}_{\ell}$. To make the final decision
  concerning the relation of two input concepts $(x,y$), we solve the
  objective function defined in Eqn. \ref{eqn:objectivefunction}.}
%
{\small
\begin{eqnarray}
\label{eqn:objectivefunction}
\ell^* & = & \text{argmax}_{\ell} \frac {1} {|\mathcal{T}_{\ell}|} \sum_{t \in \mathcal{T}_{\ell}} \lambda_{t} score(t) \\
& = & \text{argmax}_{\ell} \frac{1}{|\mathcal{T}_{\ell}|} \sum_{t \in \mathcal{T}_{\ell}} \lambda_{t} \left( \sum_{e \in t} w(e) - \sum_{k=1}^{|\mathcal{C}|} \rho_{k} d_{C_k}(t) \right) \notag
\end{eqnarray}
}
%
where $\lambda_t$ is the weight of concept network $t$, defined as the
occurrence probability of $t$ (regarding only its edges' setting) in
training data, which is augmented with additional
concepts. Eqn. (\ref{eqn:objectivefunction}) finds the best taxonomic
relation of two input concepts by computing the average score of every
group of the best concept networks representing a particular relation
with respect to a set of relational constraints.  The relational
constraints used in our experiments are manually constructed. We consider
only 3-node concept networks (i.e. $|Z| = 1$). In general, $n$-node
concept networks can be selected as relational constraints and used in
our inference model. But one can always decompose $n$-node networks
into $3$-node networks to allow greedy search in the concept
network space.

\ignore{

  To measure $\lambda_t$, a local relation classifier is used to
  predict the best relation between concepts in the concept networks
  in the training data.

  \begin{equation}
    \lambda_t = \frac{\# \text{ of occurrences of }t}{\# \text{ of
        predicted concept networks in training data}} \notag
  \end{equation}
}

\ignore{
\subsection{Constraint Selection}
\label{sec:constraintselection}

In Eqn. (\ref{eqn:objectivefunction}), a list of relational
constraints $\mathcal{C}$ is used. Recall that a relational constraint
is an invalid concept network that is not allowed to happen. In our
work, relational constraints are mined from the training data by
applying the {\em forward feature selection} technique discussed in
\cite{944968}. The algorithm starts with an empty set of constraints
$\mathcal{C}$, then grows the constraint set gradually by including in
the set the {\em best} concept network from each iteration. The {\em
  best} concept network is defined as the constraint used in
Eqn. (\ref{eqn:objectivefunction}) that improves the system's
performance most. Our {\em forward constraint selection} algorithm is
described in Fig. \ref{alg:forwardselection}. Function
\texttt{evaluate}($\mathcal{D}$, $\mathcal{L}$, $\mathcal{C}$)
evaluates the system on all examples and their related concepts in
$\mathcal{D}$ using a trained local classifier $\mathcal{L}$ with
respect to a set of constraints $\mathcal{C}$ by solving the objective
function in Eqn. (\ref{eqn:objectivefunction})

\begin{figure}[!t]
  \begin{centering}
    {\scriptsize
      \fbox{
        \begin{minipage}{6in}
          \begin{tabbing}
            {Algorithm \textsc{Forward Constraint Selection}} \\
            \qquad {\textsc{Input}}: Data set with related concepts $\mathcal{D} = \left < (x, y, \ell, \mathcal{Z}) \right >$ ~~~~~\\
            \qquad \qquad A trained local relation classifier $\mathcal{L}$. \\
            \qquad \qquad A list of concept networks $\mathcal{T} = \{ t_1, t_2, \dots, t_{m} \}$; \\
            \qquad {\textsc{Output}}: Constraint set $\mathcal{C}$. \\
            \\
            \qquad $\mathcal{C} = \emptyset$; {\em isIncreasing} = true; \\
            \qquad {\em bestAcc} = \texttt{evaluate}($\mathcal{D}$, $\mathcal{L}$, $\mathcal{C}$); \\
            \qquad While ({\em isIncreasing}) do \\
            \qquad \qquad {\em isIncreasing} = false; \\
            \qquad \qquad For each $t \in \mathcal{T}$ do \\
            \qquad \qquad \qquad $\mathcal{C} = \mathcal{C} \cup \{ t \}$; \\
            \qquad \qquad \qquad {\em acc} = \texttt{evaluates}($\mathcal{D}$, $\mathcal{L}$, $\mathcal{C}$); \\
            \qquad \qquad \qquad If ({\em acc} $>$ {\em bestAcc}) then \\
            \qquad \qquad \qquad \qquad $t^* = t$; {\em isIncreasing} = true; \\
            \qquad \qquad \qquad \qquad {\em bestAcc} $=$ {\em acc}; \\
            \qquad \qquad \qquad End if \\
            \qquad \qquad \qquad $\mathcal{C} = \mathcal{C} \backslash \{ t \}$; \\
            \qquad \qquad End for \\
            \qquad \qquad If ({\em isIncreasing}) then \\
            \qquad \qquad \qquad $\mathcal{C} = \mathcal{C} \cup \{ t^* \}$; $\mathcal{T} = \mathcal{T} \backslash \{ t^* \}$; \\
            \qquad \qquad End if \\
            \qquad End while \\
            \\
            \qquad \textsc{Return}: $\mathcal{C}$; \\
          \end{tabbing}
        \end{minipage}
      }
    }
  \end{centering}
  \caption{
    \label{alg:forwardselection}
    Forward constraint selection algorithm. Function
    \texttt{evaluate}($\mathcal{D}$, $\mathcal{L}$, $\mathcal{C}$)
    evaluates the system on all examples and their related concepts in
    $\mathcal{D}$ using a trained local classifier $\mathcal{L}$ with
    respect to a set of constraints $\mathcal{C}$ by solving
    Eqn. (\ref{eqn:objectivefunction}).}
\end{figure}

One of the inputs of the algorithm, $\mathcal{T}$, is a set of $m$
concept networks. At each iteration, all concept networks $t \in
\mathcal{T}$ are tried; at the end of the iteration, the {\em
  best} network is added to $\mathcal{C}$ and removed from
$\mathcal{T}$. In this paper, for simplicity, we only use 3-node
concept networks, thus, $m = 64$. In general, $n$-node concept
networks can also be selected as relational constraints and used in
our model by decomposing $n$-node networks into $3$-node
networks to allow greedy search performed in the concept network
space.
}

\subsection{Related Concepts Extraction}
\label{sec:rel-con-ext}

To apply our constraint-based inference model, we need to acquire
concepts additional to the input pair. In principal, with strong
relational constraints and a reasonable local taxonomic relation classifier, one
can use random concepts as additional concepts to do
inference. However, there is a high chance that a random additional
concepts will simply produce {\em no relation} with two input
concepts, and eventually will not help the inference model.
\ignore{To address this issue, we investigate different approaches to
  obtain additional concepts which are related to input concepts.}
Our experiment (see Sec. \ref{sec:experiments} shows that the proposed
inference model gets better results when doing inference with
additional concepts that are more relevant to input concepts. From now
on, we refer to additional concepts as {\em related concepts}.

\ignore{ In the first direction, as the first step to verifying the
  correctness of our proposed constraint-based inference model, we
  manually add {\em gold related concepts}, which are very related to
  input concepts. By using {\em gold related concepts}, a significant
  improvement in the system's performance is necessary to prove the
  correctness of the inference model. Our experiments (see
  Sec. \ref{sec:contr-relat-conc}) following this direction indeed
  prove that our proposed inference model is correct and accurate.  }

\begin{figure}[!t]
  \begin{centering}
    {\scriptsize
      \fbox{
        \begin{minipage}{6in}
          \begin{tabbing}
            \textsc{Yago} \textsc{Query Patterns} \\
            \qquad \textsc{Input:} concept ``$X$'' \\
            \qquad \textsc{Output:} lists of ancestors, siblings, and children of ``$X$'' ~~~~~~~~~~~~~~~~~~ \\
            \\
            \qquad \begin{tabular}{l|l|l}
              Pattern 1 & Pattern 2 & Pattern 3 \\
              \hline
              ``$X$'' \textsc{means} \texttt{?A} & ``$X$'' \textsc{means} \texttt{?A} & ``$X$'' \textsc{means} \texttt{?D} \\
              \texttt{?A} \textsc{subclassof} \texttt{?B} &  \texttt{?A} \textsc{type} \texttt{?B} & \texttt{?E} \textsc{type} \texttt{?D}  \\
              \texttt{?C} \textsc{subclassof} \texttt{?B} & \texttt{?C} \textsc{type} \texttt{?B} & \\
            \end{tabular}
            \\
            \\
            \qquad \textsc{Return:} \texttt{?B}, \texttt{?C}, \texttt{?E} as \\
            \qquad \qquad \qquad lists of ancestors, siblings, and children, resp. \\
          \end{tabbing}
        \end{minipage}
      }
    }
  \end{centering}
  \caption{Our \textsc{Yago} query patterns used to obtain related concepts for ``$X$''.}
  \label{alg:yago-query}
\end{figure}

\ignore{However, in real world applications, {\em gold related
    concepts} are not available.}

We, therefore, propose an approach that makes use of \textsc{Yago}
ontology \cite{suchanek2007WWW} to provide related concepts. It is
worth noting that \textsc{Yago} is chosen over the Wikipedia category
system used in our work because \textsc{Yago} is a clean ontology
built by carefully combining Wikipedia and lexical database
WordNet.\footnote{However, \textsc{Yago} by itself is weaker than our
  approach in identifying taxonomic relations (see
  Sec. \ref{sec:experiments}.)}

In \textsc{Yago} model, all objects (e.g. {\em cities}, {\em people},
etc.)  are represented as {\em entities}. Following the \textsc{Yago}
model, our input concepts are considered to be words. To map our input
concepts to entities in \textsc{Yago}, we use \textsc{means}
relation. Furthermore, similar entities are grouped into {\em
  classes}. This allows us to obtain direct ancestors of an entity by
using \textsc{type} relation which gives the entity's {\em
  classes}. Furthermore, we can get ancestors of a {\em class} with
\textsc{subclassof} relation. By using three relations \textsc{means},
\textsc{type} and \textsc{subclassof} in \textsc{Yago} model, we can
obtain direct ancestors, siblings, and children, if any, of an input
concept. Fig. \ref{alg:yago-query} presents three patterns that we
used to query related concepts from \textsc{Yago}. For concepts having
no children, such as {\em honda civic}, {\em bill clinton}, we simple
ignore their children.

% By default, we use this approach to provide related concepts for our
% constraint-based inference model.

\ignore{
\begin{figure}[!t]
  \begin{centering}
    {\scriptsize
      \fbox{
        \begin{minipage}{6in}
          \begin{tabbing}
            \textsc{Yago} \textsc{Query Patterns} \\
            \qquad \textsc{Input:} concept ``$X$'' \\
            \qquad \textsc{Output:} lists of ancestors, siblings, and children of ``$X$'' ~~~~~~~~~~~~~~~~~~ \\
            \\
            \qquad \textsc{Pattern 1:} \\
            \qquad \qquad ``$X$'' \textsc{means} \texttt{?A} \\
            \qquad \qquad \texttt{?A} \textsc{type} \texttt{?B} \\
            \qquad \qquad \texttt{?C} \textsc{type} \texttt{?B} \\
            \qquad \textsc{Pattern 2:} \\
            \qquad \qquad ``$X$'' \textsc{means} \texttt{?D} \\
            \qquad \qquad \texttt{?E} \textsc{type} \texttt{?D} \\
            \\
            \qquad \textsc{Return:} \texttt{?B}, \texttt{?C}, \texttt{?E} as \\
            \qquad \qquad \qquad lists of ancestors, siblings, and children, resp. \\
          \end{tabbing}
        \end{minipage}
      }
    }
  \end{centering}
  \caption{Our \textsc{Yago} query patterns used to obtain lists of direct ancestors,
    siblings, and children for concept $X$.}
  \label{alg:yago-query}
\end{figure}
}

\ignore{
In Fig. \ref{alg:yago-query}, \textsc{Pattern 1} is used to get lists
of ancestors and siblings of a given concept.  For example, with
concept ``$X$''= ``{\em honda civic}'', \textsc{Pattern 1} returns
lists of ancestors \linebreak[4] \{{\em wikicat\_1970s\_automobiles,
  wn\_car\_102958343, \linebreak[4] wikicat\_Honda\_vehicles,
  $\dots$}\}, and siblings \linebreak[4] \{{\em Ford\_Capri,
  Mercury\_Comet, Honda\_Jazz, Honda\_Accord, $\dots$}\}. The prefix
and suffix annotation of the ancestors are dropped.  In the case that
ancestors are noun phrases, we only use the head of the phrase. We use
the Noun Group parser from \cite{suchanek2007WWW} to extract the {\em
  head} of a noun phrase. In the other hand, \textsc{Pattern 2}
returns an empty list of children of ``{\em honda civic}'', because
its corresponding entity \texttt{Honda\_Civic} is at the deepest level
in \textsc{Yago} ontology. The list of children will be non-empty if
``$X$'' is some generic concept such as ``{\em actor}'', ``{\em
  president}'', etc.
}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "jupiter"
%%% End:
