In this section, we first describe the datasets. After that, we
present the experiments that show the performance of our systems and
other systems built upon existing large-scale resources. We also give
a deeper understanding about our systems by several experimental
analyses.

\subsection{Datasets}
\label{sec:dataset}
We create and use two main datasets in our experiments. 

The first dataset is generated from 40 semantic classes of almost
11,000 instances. The original semantic classes and instances were
manually constructed without much manual post-filtering and were used
to evaluate information extraction tasks as in
\cite{citeulike:1587018,pacsca-vandurme:2008:ACLMain}.  This dataset,
therefore, contains concepts having Wikipedia pages, such as {\em
  George W. Bush}, and also non-Wikipedia concepts, such as {\em hindu
  mysticism}. We create learning examples for our task by randomly
pairing the semantic classes and instances. We generate disjoint
training and test sets of 8,000 and 12,000 examples, respectively. We
call the test set of this dataset {\bf Test-I}.

The second dataset is generated from 44 semantic classes of more than
10,000 instances used in
\cite{vyas-pantel:2009:NAACLHLT09}\footnote{There were 50 semantic
  classes in the original dataset. We grouped some semantically
  similar classes for the purpose of identifying taxonomic
  relations.}. The original semantic classes and instances were
extracted from Wikipedia lists. This data, therefore, only contains
concepts having Wikipedia pages (e.g. {\em pianist}, {\em roger
  federer}). We also generate disjoint training and test sets of 8,000
and 12,000 examples and call the test set of this dataset {\bf
  Test-II}\footnote{Omitting the dataset shared URL for blind
  review.}.

\ignore{In both datasets, we have both types of closed word semantic
  class (e.g. {\em chemical element}, {\em country}) and open word
  semantic class (e.g. {\em basic food}, {\em hurricane}). Moreover,
  there are classes with proper nouns (e.g. {\em actor} with {\em Mel
    Gibson}) and classes with common nouns (e.g. {\em basic food} with
  {\em rice}, {\em milk}).}

Several semantic class names in the original data are written in short
forms (e.g. {\em chemicalelem}, {\em proglanguage}). We, therefore,
expand these names to some meaningful names which are used by all
systems in our experiments. For example, {\em terroristgroup} is
expanded to {{\em terrorist group}, {\em terrorism}}.

Table \ref{table:examples} shows some generated examples. All types of
taxonomic relations including $X \leftarrow Y$, $X \rightarrow Y$, $X
\leftrightarrow Y$, and $X \nleftrightarrow Y$ are covered with
balanced numbers of examples in all datasets.

\begin{table}[!t]
  \small
  \centering
  \begin{tabular}{r|l|l}
    %\hline
    %\multicolumn{3}{|c|}{Concept pair examples} \\
    %\hline
    Relation          & Concept $X$ & Concept $Y$    \\
    \hline
    %\hline
    $X \leftarrow Y$        & actor             & Mel Gibson           \\
    & food              & rice                 \\
    % & wine              & Champagne            \\
    \hline                               
    $X \rightarrow Y$       & Makalu            & mountain             \\
    & Monopoly          & game                 \\
    % & krooni            & currency             \\
    \hline                               
    $X \leftrightarrow Y$   & Paris             & London               \\
    & copper            & oxygen                \\
    % & Nile              & Volga                 \\
    \hline                               
    $X \nleftrightarrow Y$ & Roja              & C++                  \\
    & egg               & Vega                  \\
    % & HotBot            & autism                \\
    %\hline
  \end{tabular}
  \caption{Some examples in our data set.}
  \label{table:examples}
\end{table}


\ignore{We refer to both name of a semantic classes and its instances
  as concepts. An example in the problem of taxonomic relation
  identification is a pair of two concepts ($X$, $Y$) such as ({\em
    city}, {\em Dubai}), ({\em lead}, {\em aluminum}). We pair the
  semantic classes and instances in the original data set to create
  training and testing examples. For each original dataset, we
  randomly generate disjoint training and test sets of $12,000$ and
  $8,000$ examples, respectively. We call the test set from the first
  and the second datasets {\bf Test-I} and {\bf Test-II},
  respectively\footnote{Omitting the dataset shared URL for blind
    review.}. Table \ref{table:examples} shows some generated
  examples. All types of taxonomic relations including $X \leftarrow
  Y$, $X \rightarrow Y$, $X \leftrightarrow Y$, and $X
  \nleftrightarrow Y$ are covered with a balanced number of examples.}

\ignore{Because {\bf Test-II} contains only {\em Wikipedia concepts},
  we do not evaluate it the experiments that find replacements for
  {\em non-Wikipedia concepts}.}

To evaluate our systems, we use a snapshot of Wikipedia from July,
2008. After cleaning up and removing articles without a category
(except redirect pages), and administrative articles such as {\em
  Template}, {\em Image} and {\em Portal} pages, there are 5,503,763
articles remained. We index these articles using
Lucene\footnote{http://lucene.apache.org, version 2.3.2}.  As a
learning algorithm, we use a regularized averaged Perceptron
\cite{FreundSc99}.

\subsection{Overall Results and Comparison}
\label{sec:exp-results}

\ignore{
\begin{table*}[!t]
  \small
  \begin{center}
    \begin{tabular}{|l|c|c||c|c|}
      \hline
      \multirow{2}{*}{} & \multicolumn{2}{c}{{\bf Test-I}} & \multicolumn{2}{|c|}{{\bf Test-II}} \\
      \cline{2-5}
      &  Avg. Prec &  Avg. Rec &  Avg. Prec & Avg. Rec \\
      \hline
      Strube07  & 37.17 &  24.5 & 38.37 & 24.81 \\
      Snow06    & 50.12 & 40.19 & 41.27 & 32.19 \\
      Yago07    & 80.21 & 64.22 & 79.46 & 70.37 \\
      \hline
      {\bf Ours:}  & & & & \\
      ~~~RC        & 84.13 & 81.88 & 85.85 &  84.6 \\
      ~~~RC+BW     & 84.65 &  83.5 & 85.85 &  84.6 \\
      ~~~RC+Inf    & 85.36 & 83.33 & 87.82 & 86.87 \\
      ~~~RC+BW+Inf &  {\bf 85.9} &  {\bf 85.3} & {\bf 87.82} & {\bf 86.87} \\
      \hline
    \end{tabular}
    \caption{Evaluating and comparing performances, in average precision and recall, of the systems on {\bf
        Test-I} and {\bf Test-II}. There is no difference with {\bf BW} on {\bf Test-II} because {\bf Test-II} contains only {\em Wikipedia concepts}.}
    \label{table:compare-others}
  \end{center}
\end{table*}
}


To the best of our knowledge, no prior work directly targets the
problem of on-the-fly taxonomic relation identification. We compare
our system with three systems that we built upon different existing
resources.

\ignore{Most prior work which relates to our problem focuses on
  building lexical taxonomy or knowledge ontology
  \cite{Snow2006,wikitaxo07,suchanek2007WWW}.}

{\bf Strube07} uses a large scale taxonomy, $T_{Strube}$, which was
derived from Wikipedia \cite{wikitaxo07}. The taxonomy was created by
applying several lexical matching and methods based on connectivity in
the network to the category system in Wikipedia. We use the latest
available version of the taxonomy. It is worth noting that the
taxonomy in Strube07, $T_{Strube}$, was extracted from and is similar to
the page structure of Wikipedia \cite{wikitaxo07}. We, therefore,
first build the Wikipedia-article representation for each input
concept by following the same procedure used in our work described in
Sec. \ref{sec:learning}. The titles and categories of the articles in
the representation of each input concept are then extracted. Only
titles and their corresponding categories that are in $T_{Strube}$ are
considered. A concept is an ancestor of the other if at least one of
its titles is in the categories of the other concept. If two concepts
sharing at least one category, we predict that they are
siblings. Otherwise, there is no relation between two input
concepts. In the relation identification process, the ancestor
relation is preferred, then sibling, and finally no relation.

{\bf Snow06} uses the {\em augmented WordNet} \cite{Snow2006}. Words
in the augmented WordNet can be common nouns or proper nouns. Given
two input concepts, we first map them onto the augmented WordNet
by exact string matching. A concept is an ancestor of the other if
it can be found as an hypernym after going up some levels in the
hierarchical structure of the augmented WordNet from the other
concept. If two concepts sharing a common subsumption by going up some
levels from them, then they are considered as siblings. Otherwise,
there is no relation between the two input concepts. The order of
relation checking process is similar to Strube07: first ancestor, then
sibling, and finally no relation. We use the Java WordNet
library\footnote{http://sourceforge.net/projects/jwordnet/} to work
with the augmented WordNet.

{\bf Yago07} uses \textsc{Yago} ontology \cite{suchanek2007WWW} as the
main source of background knowledge. Because \textsc{Yago} ontology is
a combination of Wikipedia and WordNet, this system is expected to be
powerful in recognizing concept relationships. To access a concept's
ancestors and siblings, we use patterns 1 and 2 in
Figure \ref{alg:yago-query} to map a concept to the ontology and move up
on the ontology. The relation identification process is then similar
to that of Snow06.

If an input concept is a non-Wikipedia concept, all these three
systems simply pick {\em no relation} as the prediction.

Our overall algorithm, {\bf TREI}, is described in Figure
\ref{fig:rel-know-iden-alg}. We manually construct a pre-defined
list of 35 relational constraints to use in the inference model of
TREI. We also evaluate our local taxonomic relation classifier
(Sec. \ref{sec:learning}), which is referred as {\bf TREI (local)}. To
make classification decision with TREI (local), for a pair of
concepts, we choose the predicted relation with highest confidence
returned by the classifier.

\begin{table}[!t]
  \begin{center}
    \begin{tabular}{l|c|c}
      &  {\bf Test-I}  &  {\bf Test-II}  \\
      \hline
      Strube07      &   24.32  &    25.63  \\
      Snow06        &   41.97  &    36.26  \\
      Yago07        &   65.93  &    70.63  \\
      \hline
      TREI (local)  &   81.89  &     84.7  \\
      TREI          &   {\bf 85.34}  &    {\bf 86.98}  \\
      %\hline
    \end{tabular}
    \caption{Evaluating and comparing performances, in accuracy, of
      the systems on {\bf Test-I} and {\bf Test-II}. \ignore{The best
        performance of each system, by varying the number of levels to
        go up on the taxonomy or category system, from $1$ to $4$, is
        reported.} TREI (local) uses only our local classifier to
      identify taxonomic relations by choosing the relation with
      highest confidence.}
    \label{table:compare-others}
  \end{center}
\end{table}

In all systems compared, we vary the value of $K$ from $1$ to $4$ as
the levels to go up to collect information in the taxonomy or category
tree. The best result of each system is reported. Table
\ref{table:compare-others} shows the comparison of all systems
evaluated on both Test-I and Test-II.  Our systems, as shown,
significantly outperforms other systems.

We do not have special tactics to handle polysemous concepts. However,
our procedure of building Wikipedia-article representations for input
concepts described in Sec. \ref{sec:learning} ties the senses of two
input concepts to each other. We do not use this procedure in Snow06
because WordNet and Wikipedia are two different knowledge bases. We
also do not use this procedure in Yago07 because a concept is mapped
onto the \textsc{Yago} ontology by using the \textsc{means} operator
as in Pattern 1 in Figure \ref{alg:yago-query}. This cannot follow our
proposed procedure.

Observing the results shows that one of the main reasons that our
systems, TREI (local) and TREI, perform much better than
other systems is that while our machine learning-based classifier is
very flexible in extracting features of two input concepts and
predicting their taxonomic relation, other systems rely heavily on
string matching techniques, which are very inflexible, to decide the
relation. This clearly shows the limitation of using existing
structured resources to identify taxonomic relations.

In Table \ref{table:compare-others}, the improvement of TREI over TREI
(local) on Test-I shows the contribution of both normalization
procedure and our inference model that makes global prediction,
whereas the improvement on Test-II shows the contribution of the
inference model solely because Test-II contains only concepts having
corresponding Wikipedia articles.

\ignore{ Our overall system, which consists of a taxonomic relation
  classifier ({\bf RC}), a component that finds replacements ({\bf
    BW}) for {\em non-Wikipedia} concepts and an inference component
  ({\bf Inf}), is referred as {\bf RC+BW+Inf} and described in
  Figure \ref{fig:rel-know-iden-alg}. We also evaluate our system with
  only {\bf RC}, {\bf RC+Inf} and {\bf RC+BW}. In all systems
  compared, we vary the value of $K$ from $1$ to $4$ as the levels to
  go up to collect information in the taxonomy or category tree. The
  best result of each system is reported. Table
  \ref{table:compare-others} shows the comparison of all systems
  evaluated on botah {\bf Test-I} and {\bf Test-II}. Note that we do
  not find replacements for concepts in {\bf Test-II} as
  mentioned. Our systems, as shown, significantly outperform other
  systems implemented using existing sophisticated resources.  }

\subsection{Experimental Analysis}

In this section, we discuss experimental analyses to better understand
our systems.  \ignore{These analyses provide a deeper understanding of
  different aspects on our system.}

%\subsubsection{Performance on Individual Relation}

{\bf Performances on Individual Relation:} This experiment shows the
performance of our best system on individual taxonomic relation.
Table \ref{tab:ind-rel} presents the results in precision and
recall. The result shows that our system performs very well on
ancestor relation. Sibling and no relation are the most difficult
relations to identify.

\begin{table}[!t]
  \begin{center}
    \begin{tabular}{l|c|c||c|c}
      %\hline
      \multirow{2}{*}{TREI}  & \multicolumn{2}{c}{{\bf Test-I}} & \multicolumn{2}{|c}{{\bf Test-II}} \\
      \cline{2-5}
      &  Prec & Rec &  Prec &  Rec  \\
      \hline
      $X \leftarrow Y$       & 95.81 & 88.01 & 96.46 & 88.48 \\
      $X \rightarrow Y$      & 94.61 & 89.29 & 96.15 & 88.86 \\
      $X \leftrightarrow Y$  & 79.23 & 84.01 & 83.15 & 81.87 \\
      $X \nleftrightarrow Y$ & 73.94 &  79.9 & 75.54 & 88.27 \\
%      \hline
%      Average & & & & & & \\
      %\hline
    \end{tabular}
    \caption{Performance of our systems on individual taxonomic relation.}
    \label{tab:ind-rel}
  \end{center}
\end{table}

%\subsubsection{Evaluating with Special Datasets}

{\bf Evaluating with Special Datasets:} We evaluate all systems on
three special datasets derived from Test-I.  From 12,000 examples of
Test-I, we create the {\bf Wikipedia} test set consisting of $10,456$
examples, each of which has two concepts in Wikipedia. We use the rest
of $1,544$ examples with at least one non-Wikipedia concept to make
the {\bf non-Wikipedia} test set. Furthermore, if we only consider
examples with all concepts in WordNet and Wikipedia, there are $8,625$
examples of interest. We use these examples to make the {\bf WordNet}
test set. Table \ref{table:special-data} shows the comparison of the
systems on these datasets.  In this experiment, Yago07 gets better
results on the Wikipedia test set than when evaluated with Test-I.
Snow06, as expected, gets better results on the WordNet test set.
TREI still significantly outperforms other systems. The improvement of
TREI over TREI (local) on the Wikipedia and WordNet test sets shows
the contribution of the inference model, whereas the improvement on
the non-Wikipedia test set shows the contribution of normalizing input
concepts to Wikipedia.

\ignore{ By removing {\em non-Wikipedia concepts} from {\bf Test-I},
  we get $10,456$ concept pairs with concepts from {\bf
    Wikipedia}. From that, if we only consider concept pairs with
  concepts in WordNet, we have $8,625$ pairs. On the other hand, if we
  only consider {\em non-Wiki}. In {\bf Test-II}, we only have
  concepts in Wikipedia. By only consider pairs with concepts in
  WordNet, we have $7,830$ pairs, which makes {\bf Test-II-WN}.}

\ignore{
\begin{table}[!h]
  \small
  \begin{center}
    \begin{tabular}{|c||c|c|c|}
      %\hline
      %\multicolumn{5}{|c|}{Comparison on special data sets} \\
      \hline
      & {\bf Test-I-WK} & {\bf Test-I-WN} &  {\bf Test-II-WN}  \\
      \hline
      Strube07   & 24.59 & 24.13 & 26.49 \\
      Snow06     & 41.23 & 46.91 & 43.07 \\
      Yago07     & 69.95 & 70.42 & 70.37 \\
      {\bf Ours} & {\bf 91.03} &  {\bf 91.2} & {\bf 86.29} \\
      \hline
    \end{tabular}
    \caption{Performance, in accuracy, of the systems.}
    \label{table:special-data}
  \end{center}
\end{table}
}

\begin{table}[!t]
  \small
  \begin{center}
    \begin{tabular}{l|c|c|c}
      &  {\bf Wikipedia}  & {\bf WordNet}  &  {\bf non-Wikipedia}  \\
      \hline
      Strube07  &      24.59         &     24.13      & 21.18                 \\
      Snow06    &      41.23         &     46.91      & 34.46                 \\
      Yago07    &      69.95         &     70.42      & 34.26                 \\
      \hline
      TREI (local) &   89.37         &     89.72      & 31.22                 \\
      TREI      &      {\bf 91.03}   &     {\bf 91.2} & {\bf 45.21}           \\
      %\hline
    \end{tabular}
    \caption{Performance of the systems on special datasets, in
      accuracy. For examples in the non-Wikipedia test set, TREI
      (local) simply returns sibling relation.}
    \label{table:special-data}
  \end{center}
\end{table}


%\subsubsection{Contribution of Related Concepts in Inference}
%\label{sec:contr-relat-conc}
{\bf Contribution of Related Concepts in Inference:} We evaluate TREI
when provided with related concepts from another source different than
\textsc{Yago}.  To do this, we use the original data which was used to
generate Test-I. For each concept in the examples of Test-I, we get
its ancestors, siblings, children, if any, from the original data and
use them as related concepts in the inference model. This system is
referred as {\bf TREI (Gold Infer.)}. Table
\ref{table:related-concepts} shows the results of TREI and TREI (Gold
Infer.) on different $K$ as the number of levels to go up on the
Wikipedia category system.  We see that TREI gets better results when
doing inference with better related concepts. In this experiment, two
systems use the same number of related concepts.

\begin{table}[!t]
  \small
  \begin{center}
    \begin{tabular}{l|c|c|c|c}
      &    $K$=1  &    $K$=2  &    $K$=3  &    $K$=4  \\
      \hline
      TREI             &  82.93  &  85.34  &  85.23  &  83.95  \\
      TREI (Gold Infer.)  &  83.46  &  86.18  &   85.9  &  84.93  \\
    \end{tabular}
    \caption{Evaluating TREI with different sources providing related
      concepts to do inference. $K$ is the number of levels to go up
      on the Wikipedia category system.}
    \label{table:related-concepts}
  \end{center}
\end{table}

\ignore{
\begin{figure}[t]
  \centering
  \includegraphics[totalheight=0.21\textheight]{relatedconcepts4}
  \caption{Relation between our system performance and the number of
    related concepts used in the constraint-based inference model.}
  \label{fig:related-concepts}
\end{figure}
}

\ignore{ In Sec. \ref{sec:rel-con-ext},  we propose the intuition that
  the more  relevant to the  input concepts the related  concepts are,
  the  better performance  the  system gets.  In  this experiment,  we
  empirically show the correctness  of the intuition by evaluating our
  best    system    with    inference    model   on    {\bf    Test-I}
  dataset.  Figure \ref{fig:related-concepts} shows  the results  of the
  systems using  the inference  model with related  concepts extracted
  from the original dataset and from \textsc{Yago}.  }

%%%%%%%%%%%%%%%
% Old writing %
%%%%%%%%%%%%%%%

\ignore{Specifically, examples are created with the following
  guidelines.

  \begin{itemize}
  \item $X \leftarrow Y$ examples: For each semantic class, we pair
    the name of the class with its instances. These examples have the
    general form ({\em semantic class X}, {\em child of X}).
  \item $X \rightarrow Y$ examples: These examples have the general
    form ({\em concept X}, {\em semantic class of X}).
  \item $X \leftrightarrow Y$ examples: The general form of these
    examples is ({\em concept X}, {\em concept Y}), where $X$ and $Y$
    are two instances of a semantic class, and $X \ne Y$.
  \item $X \nleftrightarrow Y$ examples: To make examples with two
    concepts having no relation, we pair either a semantic class name
    and an instance of another semantic class (and vice versa), or an
    instance in a semantic class and another instance in other
    classes.
  \end{itemize}
}

\ignore{We refer to the 12,000-example test set as the {\em TestAll}
  test set. From the training set, we discard examples with one or
  both concepts not in Wikipedia ({\em non-Wikipedia examples}). This
  results in a training set of $6,959$ examples.  It is important to
  note that, we do not discard {\em non-Wikipedia} examples in the
  {\em TestAll} test set. Table \ref{tab:detail-dataset} shows the
  statistics of the training and test data.

  \begin{table}[!t]
    \small
    \begin{center}
      \begin{tabular}{|l||c|c|c|c|c|}
        \hline
        Data & $X \leftarrow Y$ & $X \rightarrow Y$  & $X \leftrightarrow Y$  & $X \nleftrightarrow Y$ & Total \\
        \hline
        \hline
        {\em Training}  & 1,739 & 1,754 & 1,664 & 1,802 & 6,959\\
        % {\em TestWiki} & 2,684 & 2,654 & 2,483 & 2,635 & 10,456 \\
        {\em TestAll} & 3,045 & 3,025 & 2,965 & 2,965 & 12,000 \\
        \hline
      \end{tabular}
      \caption{Details of the training and test sets with the number
        of examples in each relation class. The {\em Training} set
        contains only examples in Wikipedia. {\em TestAll} includes
        {\em non-Wikipedia} examples.}
      \label{tab:detail-dataset}
    \end{center}
  \end{table}
}

\ignore{Each semantic class is an incomplete set of representative
  instances and has about 272 instances in average. The smallest class
  is {\em search engine} with 25 instances, and the largest class is
  {\em actor} with 1500 instances. Table \ref{table:class-instance}
  shows a snippet of the dataset. We have both types of closed word
  semantic class (e.g. {\em chemical element}, {\em country}) and open
  word semantic class (e.g. {\em basic food}, {\em
    hurricane}). Moreover, there are classes with proper nouns
  (e.g. {\em actor} with {\em Mel Gibson}) and classes with common
  nouns (e.g. {\em basic food} with {\em rice}, {\em milk}).

  \begin{table}[!t]
    \small
    \centering
    \begin{tabular}{|r|l|}
      \hline
      \multicolumn{2}{|c|}{A snapshot of the original dataset} \\
      \hline
      \textbf{Semantic class (Size)} & \textbf{Examples of Instances} \\
      \hline
      \hline
      basic food (155) & rice, milk, eggs, beans, fish \\
      \hline
      chemical element (118) & lead, copper, aluminum, calcium \\
      \hline
      city (589) & San Francisco, Dubai, Chicago \\
      \hline
      disease (209) & arthritis, hypertension, influenza \\
      \hline
      actor (1500) & Kate Hudson, Mel Gibson \\
      \hline
    \end{tabular}
    \caption{A snippet of 40 semantic classes with instances. 
      The class names in the original dataset ({\em basicfood}, 
      {\em chemicalelem}) were presented in a meaningful form
      as shown in the left column.}
    \label{table:class-instance}
  \end{table}
}

\ignore{ Lucene is a high-performance text search library written in
  Java and widely used as an off-the-shelf IR system.}

\ignore{
  \begin{table}[!t]
    \small
    \begin{center}
      \begin{tabular}{|c||c|c|c|c|}
        \hline
        \multicolumn{5}{|c|}{Overall results and comparison} \\
        \hline
        $K$  &  \textsc{Strube 07}  &  \textsc{Snow 06}  &  \textsc{Yago 07}  &  \textsc{Ours}            \\
        \hline
        1  &      23.83  &    40.24  &           64.43  &  \textbf{82.13}  \\
        2  &      23.84  &    42.63  &           63.94  &  \textbf{84.79}  \\
        3  &      23.88  &    40.96  &           62.02  &  \textbf{85.07}  \\
        4  &      24.32  &    40.65  &           60.57  &  \textbf{84.08}  \\
        \hline
      \end{tabular}
      \caption{Performance of our overall system compared to other
        systems. Performances are measured by accuracy. The {\em
          TestAll} test set with $12,000$ examples is used in this
        experiment. $K$ is the number of levels to climb up in the
        hierarchical structure of knowledge sources as described in the
        text.}
      \label{table:compare-others}
    \end{center}
  \end{table}
}

\ignore{Our system also uses Wikipedia as its main background
  knowledge. However, ours is superior to other systems because it
  employs advance machine learning techniques along with a powerful
  and effective constraint-based inference model.}

\ignore{
  \begin{enumerate}

  \item \textsc{Strube 07} uses a large scale taxonomy which was
    derived from Wikipedia \cite{wikitaxo07}, as the background
    knowledge. The taxonomy was created by applying several lexical
    matching and methods based on connectivity in the network to the
    category system in Wikipedia. As a result, the taxonomy contains a
    large amount of subsumption, i.e. {\em isa}, relations
    \cite{wikitaxo07}. Given an input with two concepts $X$ and $Y$,
    using the taxonomy, $X$ is an ancestor of $Y$ if one of the
    articles about $Y$ is subsumed by an article about $Y$, using {\em
      isa} links of articles, up to $K$ levels in the taxonomy.
    Similarly, for the case that $Y$ is an ancestor of $X$. If $X$ and
    $Y$ share a common ancestor within $K$ levels in the taxonomy,
    they are considered siblings. We first apply our concept
    disambiguation algorithm on given concepts and then mount them
    onto the taxonomy to infer the relations. The taxonomy used in
    this experiment is in the latest version from March,
    2008\footnote{Private communication with Michael Strube and Simone
      Paolo Ponzetto, 2009.}.

  \item \textsc{Snow 06} uses the {\em augmented WordNet}
    \cite{ilprints665,Snow2006} as background knowledge. To build the
    {\em augmented WordNet}, the authors first identified
    lexico-syntactic patterns indicative of hypernymy from corpora and
    use them to extract candidate noun pairs that may hold the
    hypernym relation. A trained classifier is applied on these noun
    pairs to recognize the pairs holding hypernym relation. Starting
    from WordNet-2.1 \cite{Fellbaum98}, the latest version of the
    augmented WordNet has augmented 400,000 synsets. Words that are
    added into the augmented WordNet can be common nouns or proper
    nouns. The augmented WordNet can serve as effective background
    knowledge for identifying the relational knowledge of interest by
    looking for the input concepts in the augmented WordNet tree in all
    possible senses of the concepts and then inferring their
    relationship. We also vary the value of $K$ as the number of
    levels to go up on the WordNet tree from input concepts to find
    their common subsumptions.

  \item \textsc{Yago 07} uses YAGO ontology \cite{suchanek2007WWW} as
    the main source of background knowledge. Because YAGO ontology is
    a combination of Wikipedia and WordNet (see our brief description
    in Sec. \ref{sec:rel-con-ext}), this system is expected to be
    powerful in recognizing concept relationships. To access a
    concept's ancestors and siblings, we combine \textsc{Pattern 1} in
    Figure \ref{alg:yago-query} and the \textsc{subClassOf} relation in
    YAGO model to go up on the ontology. The \textsc{subClassOf}
    relation can be cascaded $K$ times to climb up in the ontology.

  \end{enumerate}
}

\ignore{
Because the systems in the previous experiments do not use the same
source of background knowledge, we, furthermore, perform experiments
in which these systems are compared with different data sets covered
in different knowledge sources. This experiment provides a fair
comparison for systems that use different resources as their
background knowledge. New data sets are derived from the {\em TestAll}
test set. The first derived data set is {\em TestWiki} which contains
only {\em Wikipedia concepts}. By removing all examples with {\em
  non-Wikipedia concept}, there are $10,456$ examples remained in {\em
  TestWiki}. The second derived data set is {\em TestWn}, which
contains only concepts in the {\em augmented WordNet}
\cite{ilprints665,Snow2006}.  {\em TestWn} has $8,625$ examples after
dropping $3,375$ examples with concepts not in the {\em augmented
  WordNet} from {\em TestAll}.  The results of this experiment are
shown in Table \ref{table:exp-diff-test}.  We only report the best
results achieved by the systems on different test sets. Our system
still significantly outperforms other systems when compared on
specific data sets.

\begin{table}[!t]
  \small
  \begin{center}
    \begin{tabular}{|c||c|c|c|c|}
      \hline
      \multicolumn{5}{|c|}{Comparison on special data sets} \\
      \hline
      Data set &  \textsc{Strube 07}  &  \textsc{Snow 06}  &  \textsc{Yago 07}  &  \textsc{Ours}   \\
      \hline
      {\em TestWiki}  &               24.59  &             44.34  &             70.29  &  \textbf{90.92}  \\
      {\em TestWn}    &               24.13  &             47.79  &             70.81  &  \textbf{90.76}  \\
      \hline
    \end{tabular}
    \caption{Comparison of systems' performance with different test
      sets derived from the {\em TestAll} test set. {\em TestWiki}
      contains $10,456$ examples in Wikipedia, and {\em TestWn}
      contains $8,625$ examples in the {\em augmented WordNet}. The
      best performance of each system (by varying $K$) is reported.}
    \label{table:exp-diff-test}
  \end{center}
\end{table}
}

\ignore{
\begin{table*}[!t]
  \begin{center}
    \begin{tabular}{|l||c|c|c|}
      \hline
      \multicolumn{4}{|c|}{Constribution of the components in our system} \\
      \hline
      &         Results on $10,456$  &  Results on $1,544$              &         Overall results  \\
      System &  {\em Wikipedia examples}  &  {\em non-Wikipedia examples}  &  on $12,000$ examples  \\
      \hline
      \hline
      Baseline        &                       88.79  &                      31.22       &                   81.38  \\
      \hline
      w/o Inference   &                       88.79  &                      44.88       &                   83.14  \\
      \hline
      with Inference  &                       \textbf{90.92}  &                      \textbf{45.40}       &                   \textbf{85.07}  \\
      \hline
    \end{tabular}
    \caption{\small Contributions of the components in our system evaluated
      on three types of data. The first column shows results only on
      examples with concepts that appear in Wikipedia. The second
      column shows results on examples having concepts that do not
      appear in Wikipedia, and thus exhibits the power of our method
      to extend outside Wikipedia. The third column shows results on
      the overall data set, that consists of around 13\% of the
      concepts pairs that are outside Wikipedia. The {\em Baseline}
      system is our local relation classifier that uses neither the
      approach of finding replacements for {\em non-Wikipedia
        concepts} nor the inference model. The baseline on {\em
        non-Wikipedia examples} is computed by assuming sibling
      relation all the time. By adding the component of finding
      replacements for {\em non-Wikipedia concepts}, we have the {\em
        w/o Inference} system.}
    \label{tab:w-wo-infer}
  \end{center}
\end{table*}
}


\ignore{
  \subsubsection{Contributions of the Components in our System}

  In this experiment, we show the contribution of the components in
  our overall algorithm (see Figure \ref{fig:rel-know-iden-alg}). We use
  different data sets in this experiments to study the variation in
  behavior of our system. The first data set is the {\em TestWiki} in
  the previous experiment. This test set contains $10,456$ examples
  with {\em Wikipedia concepts}. These are the remained examples after
  dropping {\em non-Wikipedia concepts} from $12,000$ examples in the
  {\em TestAll} test set. The second test set contain only $1,544$
  concept pairs with at least one {\em non-Wikipedia} concept in
  each. In other words, the second test set is the complement of the
  {\em TestWiki} test set with respect to the {\em TestAll} test
  set. The third test set is the {\em TestAll} test set. The results
  are shown in Table \ref{tab:w-wo-infer}.

  In Table \ref{tab:w-wo-infer}, the {\em Baseline} system is our local
  relation classifier that uses neither the component finding
  replacements for {\em non-Wikipedia concept} nor the
  constraint-based inference model. The baseline on {\em non-Wikipedia
    examples} is computed by assuming sibling relation all the
  time. In the {\em w/o Inference} row, we evaluate our system with
  the contribution of the component finding replacements for {\em
    non-Wikipedia concepts}, but inference model. The result on
  $10,456$ {\em Wikipedia examples} remains the same because, all of
  concepts in these examples are in Wikipedia, the system does not
  need to find replacement concepts. The significant improvement in
  the results on $1,544$ {\em non-Wikipedia examples} shows the
  effectiveness of our approach in finding replacements for {\em
    non-Wikipedia concepts}. Overall, we obtain almost $2\%$ of
  improvement in accuracy after adding the finding replacement
  component. The third experiment is carried out by evaluating our
  overall algorithm in Figure \ref{fig:rel-know-iden-alg} on all test
  sets. The inference process significantly improve the results of our
  system on $10,456$ {\em Wikipedia examples}. It also gains
  improvements on the {\em non-Wikipedia examples} so that in overall,
  by adding the constraint-based inference component, the system
  improves $\sim 2\%$ in accuracy. Overall, our system achieves
  significant improvement reaching to $85.07\%$ accuracy in overall by
  adding in all components, compared to $81.38\%$ accuracy gained by
  the {\em Baseline} system.

  \begin{table*}[!t]
    \small
    \begin{center}
      \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        No.  &  $X$             &  $Y$          &  True    &  LocalClassifier       &  \multicolumn{2}{|c|}{Replacement Needed?}  &  withInference \\ \cline{6-7}
        &                  &               &  Label   &  Prediction                   &  $X$                  &  $Y$                     &  Prediction           \\
        \hline
        \hline
        1&city                    &lisbon                   &$X \leftarrow Y$      &$X \leftarrow Y$      &(no)        &(no)         &$X \leftarrow Y$  \\
        2&{\em bartlomiej strobel}&painter                  &$X \rightarrow Y$     &$X \rightarrow Y$     &drew struzan&(no)         &$X \rightarrow Y$ \\
        3&taiwan                  &singapore                &$X \leftrightarrow Y$ &$X \leftarrow Y$               &(no)        &(no)         &$X \leftrightarrow Y$ \\
        4&jean ingres             &{\em harald slott-moller}&$X \leftrightarrow Y$ &$X \nleftrightarrow Y$         &(no)        &george inness&$X \leftrightarrow Y$ \\
        5&{\em southern herald}   &newspaper                &$X \rightarrow Y$     &$X \leftrightarrow Y$          &liberty     &(no)         &$X \rightarrow Y$  \\
        6&somalia                 &hurricane                &$X \nleftrightarrow Y$&$X \nleftrightarrow Y$&(no)        &(no)         &$X \leftrightarrow Y$  \\
        \hline
      \end{tabular}
      \caption{Relationship prediction examples from {\em TestAll}
        test set. Concepts in {\em italic font} are not in Wikipedia,
        and they are replaced by other {\em Wikipedia concepts} as
        shown.}
      \label{table:pre-examples}
    \end{center}
  \end{table*}
}

\ignore{
In our inference process, we used related concepts added to form
relational constraints in concept networks. The relational constraints
will then enforce concept networks to eliminate violated ones and pick
the best concept network available. From our inference model, we make
two following claims: (1) The more relevant to the input concepts the
related concepts are, the better performance the system gets, and (2)
the more related concepts added to the inference process, the better
performance the system gets.

To prove the first claim, we use the original data of forty semantic
classes (see Sec. \ref{sec:dataset}) to provide {\em gold related
  concepts} to the inference process as discussed in
Sec. \ref{sec:rel-con-ext}. Our experiment shows that by using this
approach, the performance of our best system reaches to $86.52\%$ in
accuracy on the {\em TestAll} test set, compared to $85.07\%$ accuracy
obtained when using another knowledge source to provide related
concepts. For the second claim, we evaluate our system with several
numbers of related concepts added to the inference process. In this
experiment, we use both the {\em gold related concepts} provided by
the original dataset and the {\em YAGO related concepts} extracted
from YAGO ontology (see Sec. \ref{sec:rel-con-ext}) for the inference
process.

Figure \ref{fig:related-concepts} shows the improvement in the
performance of our system with respect to the quality of the related
concepts used and also the number of related concepts. This experiment
is done on the {\em TestAll} test set. It is shown clearly that the
related concepts from gold data outperform the related concepts
extract from the other source. Due to the quality of the related
concepts extracted from YAGO ontology, sometimes, adding more concepts
hurts the performance. We use $K=2$ and $K=3$ in this
experiment\footnote{$K$ is the number of levels to go up in the
  Wikipedia category system to extract features for input concept
  pairs.}. The related concepts include the ancestors, siblings, and
children of an input concept. For simplicity, in this experiment we
only report the total number of related concepts extracted for two
input concepts of an example.
}

\ignore{
  \subsubsection{System's Output Examples}

  Table \ref{table:pre-examples} shows some output examples of our
  overall system in Fig. \ref{fig:rel-know-iden-alg} (see column {\em
    withInference}). We also present the predictions made by the
  system without inference component ({\em LocalClassifier}). Concepts
  which are not in Wikipedia will be replaced by {\em Wikipedia
    concepts} ({\em Replacement Needed?}). Example \#1 and \#2 show
  two concept pairs correctly classified by both {\em LocalClassifier}
  and {\em with Inference}. Especially, in \#2, that concept {\em
    bartlomiej strobel} is replaced with {\em drew struzan} helps the
  systems make correct predictions. In examples \#3, {\em
    LocalClassifier} make incorrect predictions, but {\em
    withInference} remedies this. Example \#4 and \#5 show two cases
  where two {\em non-Wikipedia concepts} are correctly replaces with
  two corresponding {\em Wikipedia concepts}, but {\em
    LocalClassifier} still makes incorrect prediction. However, {\em
    withInference}, with its power of internal reasoning using
  relational constraints, proves to be successful. Example \#6 is an
  example where {\em LocalClassifier} makes correct prediction, but
  {\em withInference} does not. This is probably because of bad
  related concepts extracted for the two input concepts.  }


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "jupiter"
%%% End: 
