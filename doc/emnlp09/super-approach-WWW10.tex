Given two input concepts ($X$, $Y$), we first disambiguate them to get
lists of relevant Wikipedia articles. We then extract features of
disambiguated concepts and train a supervised multi-class classifier
to classify relations between them.

\begin{table*}[!t]
  \tiny
  \centering
  \begin{tabular}{|p{1.0in}|p{3.5in}|p{2.0in}|}
    \hline
    {\bf Concept/Title} & {\bf Text} & {\bf Categories} \\
    \hline
    \hline
    President of the United States & \textit{The President of the United States is the head of state and head of government of the United States and is the highest political official in the United States by influence and recognition. The President leads the executive branch of the federal government and is one of only two elected members of the executive branch...} & \textit{Presidents of the United States, Presidency of the United States} \\
    \hline
    George W. Bush & \textit{George Walker Bush; born July 6, 1946) served as the 43rd President of the United States from 2001 to 2009. He was the 46th Governor of Texas from 1995 to 2000 before being sworn in as President on January 20, 2001...} & \textit{Children of Presidents of the United States, Governors of Texas, Presidents of the United States, Texas Republicans...} \\
    \hline
    Gerald Ford & \textit{Gerald Rudolff Ford (born Leslie Lynch King, Jr.) (July 14, 1913 â€“ December 26, 2006) was the 38th President of the United States, serving from 1974 to 1977, and the 40th Vice President of the United States serving from 1973 to 1974.} & \textit{Presidents of the United States, Vice Presidents of the United States, Republican Party (United States) presidential nominees...} \\
    \hline
  \end{tabular}
  \caption{Examples of texts and categories of the concepts from Wikipedia: {\em President of the United States}, {\em George W. Bush} and {\em Gerald Ford}.}
  \label{tab:snippets}
\end{table*}

\subsection{Concept Disambiguation}
\label{sec:conc-disamb}

\ignore{In this paper, we address the problem of identifying relation
  between concepts.  Our problem is different than other previous work
  on relation extraction. Most of traditional relation extraction
  tasks often focus on extracting all possible entity pairs that
  follow some pre-defined relations (e.g. LocatedIn, BornInYear,
  AuthorOf, etc.) in big corpora
  \cite{banko-etzioni:2008:ACLMain}. Some other previous work
  extracting all possible entity pairs and relations from big corpora
  with a list of same class entities as the start point
  \cite{davidov-rappoport:2008:ACLMain2}. Another direction of
  relation extraction is to extract all possible instances of a given
  class, or in the same class of given instances
  \cite{WangCohen09,kozareva-riloff-hovy:2008:ACLMain}.}

\ignore{In this paper, we limit the relations of interest to
  \emph{Ancestor}, \emph{Sibling}, or \emph{None} relations.}

Given two input concepts ($X$, $Y$), we must first disambiguate
them. The more accurate the disambiguation is, the more precise the
relation recognition is. For example, the concept \emph{Ford} in the
concept pair \emph{(Bush, Ford)} may refer to several concepts, such
as \emph{Ford Motor Company}, president \emph{Gerald Ford}, or
\emph{Ford, Wisconsin - USA}. Our approach is motivated by observing
that the concept \emph{Bush} can be used to disambiguate \emph{Ford}
and vice versa. Ultimately, we can place articles about \emph{George
  W. Bush} and \emph{Gerald Ford} the top of a retrieved articles list
for the two input concepts \emph{Bush} and \emph{Ford}.

Figure \ref{fig:entity-disamb} shows the concept disambiguation
algorithm. 

\begin{figure}[!t]
  \begin{centering}
    {\scriptsize
      \fbox{
        \begin{minipage}{6in} 
          \begin{tabbing}
            {\textsc{Concept Disambiguation Algorithm}} \\
            \qquad {\textsc{Input}}: Concept pair ($X$, $Y$); Number of top tokens $N$ ~~~~~~~ \\
            \qquad {\textsc{Output}}: Two list of relevant articles for $X$ and $Y$. \\
            \\
            \qquad Query $q \leftarrow $ Concatenating $X$ and $Y$; \\
            \qquad $\mathcal{L} = \texttt{ESA}(q)$; \\ %// return a list of relevant articles to $X$ and $Y$ \\
            \qquad $\mathcal{C} = \texttt{getCategories}(\mathcal{L})$; \\
            \qquad $\mathcal{T} = \texttt{tokenize}(\mathcal{C})$; \\
            \qquad $\mathcal{T}_{top} = \texttt{topTfidf}(\mathcal{T}, N)$; \\ %// pick top $N$ tokens\\
            \\
            \qquad Query $q_X \leftarrow $ Concatenating $X$ and tokens in $\mathcal{T}_{top}$; \\
            \qquad Query $q_Y \leftarrow $ Concatenating $Y$ and tokens in $\mathcal{T}_{top}$; \\
            \qquad $\mathcal{A}_X = \texttt{search}(q_X)$; \\
            \qquad $\mathcal{A}_Y = \texttt{search}(q_Y)$; \\
            \\
            \qquad \textsc{Return}: $\mathcal{A}_X$ and $\mathcal{A}_Y$; \\
          \end{tabbing}
        \end{minipage}
      }
  }
\end{centering}
\caption{Concept disambiguation algorithm. The algorithm takes two
  input concepts, disambiguates them and returns a list of relevant
  Wikipedia articles for each.}
\label{fig:entity-disamb}
\end{figure}

Function \texttt{ESA}({\em query}) retrieves the most semantically
relevant articles in Wikipedia to {\em query}.  We use Explicit
Semantic Analysis (ESA) \cite{GabrilovichMa07} to retrieve
semantically relevant articles in Wikipedia for two given
concepts. For all articles in the relevant list, we extract and keep
their categories by the function \texttt{getCategories}({\em list}). All
categories are then tokenized and weighted using the TF-IDF
score. Only top $N$ tokens, which are different from the two input
concepts, are considered. Each input concept is then concatenated with
the top weighted tokens to create a new query that, in turn, is the
input for the \texttt{search}({\em query}) function. Our search
function returns relevant articles in Wikipedia for the input {\em
  query}. The lists of relevant articles are later used to provide
features for the local relation classifier. If a returned list is
empty, its corresponding concept is considered as a {\em non-Wikipedia
  concept}.

\subsection{Learning Relations}
\label{sec:learner}

We first extract expressive features associated with the two input
concept pairs. These features are necessarily independent of the
semantic class of the given concepts so that our classifier can
generalize well. Recall that, after concept disambiguation,
each input concept is associated with a list of relevant articles in
Wikipedia. In other words, from these articles, an input concept is
represented with several Wikipedia titles, texts, and categories. From
now on, we use {\em the titles of concept $X$} to refer to the titles
of the articles associated with concept $X$; similarly for {\em the
  texts of concept $X$}, and {\em the categories of concept $X$}. As a
learning algorithm, we use a regularized averaged Perceptron
\cite{FreundSc99}. The features selected for our learning problem
include the words used in the associated articles, the association
information between two concepts, and the overlap ratios of important
information between the concepts and the articles. We describe these
features below.

\subsubsection{Bags of Words}

One of the most important features in recognizing relations between
concepts is the words used in associated articles. An article in
Wikipedia typically contains three main components: title, text, and
categories. The title distinguishes a concept from other concepts in
Wikipedia; the text describes the concept; and the categories classify
the concept into one or more concept groups which can be further
categorized. To collect the categories for a concept, we take the
categories of its associated articles and go up $K$ levels in the
Wikipedia category system. In our experiments, we use abstracts of
Wikipedia articles instead of whole texts.

We define four bag-of-word features associated with any two given
concepts $X$ and $Y$: (1) the degree of similarity between the texts
of concept $X$ and the categories of concept $Y$, (2) the similarity
between the categories of $X$ and the texts of $Y$, (3) the similarity
between the text of $X$ and the text of $Y$, and (4) the similarity
between the categories of $X$ and the categories of $Y$.

Table \ref{tab:snippets} shows three related concepts and their
associated articles in Wikipedia\footnote{Note that each concept can
  be associated with more than one article in Wikipedia depending on
  the list of relevant articles returned by the concept disambiguation
  algorithm.}. We see the overlap in the words used in the title, the
text and the categories of these three concepts. The overlap in word
usage determines the degree of similarity of concepts holding
relations of interest.

There are several ways to calculate the degree of similarity between
two text fragments \cite{mohler-mihalcea:2009:EACL}. We use the cosine
similarity metric. The following equation shows the cosine similarity
metric applied to the term vectors $T_1$ and $T_2$ of two text
fragments.

\begin{equation}
\label{eq:1}
     Sim(T_1, T_2) = \frac{T_1 \cdot T_2}{\left\| T_1 \right\| \left\| T_2 \right\|} = \frac{\sum_{i=1}^{N}x_iy_i}{\sqrt{\sum_{j=1}^{N}x_j} \sqrt{\sum_{k=1}^{N}y_k}}  \notag
\end{equation}


where $N$ is the vocabulary size, and $x_i$, $y_i$ are two indicator
values in $T_1$ and $T_2$, respectively.

\subsubsection{Association Information}
Another useful feature that can help in recognizing the relation
between two concepts is their association information. We capture the
association information between two concepts $X$ and $Y$ by measuring
their pointwise mutual information, defined in the following equation.

\begin{equation}
\label{eq:2}
  PMI(X, Y) = log \frac{p(X, Y)}{p(X)p(Y)} = log \frac{\frac{f(X,Y)}{N}}{\frac{f(X)}{N} \frac{f(Y)}{N}} = log\frac{N f(X,Y)}{f(X)f(Y)} \notag
\end{equation}

% \begin{equation}
% \label{eq:2}
%   PMI(x, y) = log \frac{p(x, y)}{p(x)p(y)} = log \frac{\frac{C(x,y)}{N}}{\frac{C(x)}{N}\frac{C(y)}{N}} = log\frac{N \times C(x,y)}{C(x)C(y)}
% \end{equation}

where $X$ and $Y$ are two input entities, $N$ is the total number of
documents in Wikipedia, and $f(.)$ is a function returning document
frequency. We measure association information at the document level.

\subsubsection{Overlap Ratio}
Wikipedia articles are not only helpful for definitions and
explanations of concepts, they are also useful for categorizing
concepts into groups and topics. We capture the fact that the titles
of a concept often overlap with the categories of its
descendants. For examples, the title (and also the concept itself)
{\em Presidents of the United States} overlap with one of the
categories of the concepts {\em George W. Bush} and {\em Gerald Ford}
(see Tab. \ref{tab:snippets}.) We measure this overlap
as the ratio of {\em common phrases} used in the titles of concept $X$
and the categories of concept $Y$. In our context, a phrase is
considered to be a {\em common phrase} if it appears in the titles of
$X$ and the categories of $Y$ and is one of the following: (1) the
{\em whole string} of a category of $Y$, or (2) the {\em head} in its
root form of a category of $Y$, or (3) the {\em post-modifier} of a
category of $Y$.
%\begin{itemize}
%\item the {\em whole string} of a category of $Y$, or
%\item the {\em head} in its root form of a category of $Y$, or
%\item the {\em post-modifier} of a category of $Y$.
%\end{itemize}
We use the Noun Group Parser from \cite{suchanek2007WWW} to extract
the {\em head} and {\em post-modifier} from a category. For example,
one of the categories of an article about \emph{Chicago} is
\emph{Cities in Illinois}. This category can be parsed into a head in
its root form \emph{City}, and a post-modifier \emph{Illinois}. Given
two entity pairs \emph{(Illinois, Chicago)} and \emph{(City,
  Chicago)}, we can recognize that \emph{Illinois} and \emph{City}
match the category {\em Cities in Illinois} of concept {\em
  Chicago}. Therefore, we have a strong indication that {\em Chicago}
is a descendant of both {\em Illinois} and {\em City}. We measure the
overlap ratio between the titles of concept $X$ and the categories of
concept $Y$ up to $K$ levels in the Wikipedia category system by using
the Jaccard similarity coefficient.

\begin{equation}
  \label{eq:3}
  \sigma_{ttl}^K(X,Y) = \frac{\left| L_X \cap \mathcal{P}_Y^K \right|}{\left| L_X \cup \mathcal{P}_Y^K \right|} \notag
\end{equation}

where $L_X$ is the set of the titles of $X$, and $\mathcal{P}_Y$ is
the union of the category strings, the heads of the categories, and
the post-modifiers of the categories of $Y$. Similarly, we also use
the ratio $\sigma_{ttl}^K(Y,X)$ as one of our features.

We also use a feature that captures the overlap ratio of {\em common
  phrases} between the categories of two given entities. However, to
capture the sibling relation, we do not use the {\em post-modifier} of
the categories. The Jaccard similarity coefficient for overlap ratio
between the categories of two concepts $X$ and $Y$ is shown in the
following equation.

\begin{equation}
  \label{eq:4}
  \sigma_{cat}^K(X,Y) = \frac{\left| \mathcal{Q}_X^K \cap \mathcal{Q}_Y^K \right|}{\left| \mathcal{Q}_X^K \cup \mathcal{Q}_Y^K \right|} \notag
\end{equation}

where $\mathcal{Q}_{(.)}^K$ is the union of the category strings and
the heads of the categories. The categories are collected by going up
$K$ levels in the Wikipedia category system.

All of the features described above are used to train a local
multi-class classifier to recognize relations between concepts.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "jupiter"
%%% End: 
