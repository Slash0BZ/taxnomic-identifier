Many data and knowledge management problems require some sort of
textual inference. These range from query expansion and interpretation
in information retrieval to query schema matching and question
answering. Especially, in advanced web search and contextual
advertising, textual inference plays an important role as the core
technique to finding related information that can attract users'
interest. For instance, while searching for the reviews of {\em Nikon
  D90}, one may also be interested in reading the reviews for other
{\em Nikon's cameras}, or {\em cameras} in general. To satisfy users'
interest, advanced search engines need to be equipped with textual
inference techniques that can perform search on related concepts in
the index of web documents. Textual inference requires the use of
large amounts of background knowledge. For example, it may be
important to know that a {\em blue Toyota} is not a {\em red Toyota}
nor a {\em blue Honda} but that all are cars, and even Japanese made
cars.

In this paper, we present a novel approach to a fundamental problem
that recognizes basic relations between two given concepts. This view
of relations is different from that of Open Information Extraction
\cite{BCSBE07} and On-Demand Information Extraction \cite{Sekine06},
which aim at extracting large databases of open-ended facts building
on the concepts occurring together in some local context. It is also
different from the supervised relation extraction\cite{RothYi04}
effort which requires additional supervised data to learn new
relations.

While there has been a large body of work in the direction of extracting all {\em easy to find} facts in a given
corpus~\cite{banko-etzioni:2008:ACLMain,davidov-rappoport:2008:ACLMain2,pacsca-vandurme:2008:ACLMain},
%%% Need to put in cited.bib and change the style
it is not clear how to make use of these if one is interested in determining whether two given concepts,
{\em A} and {\em B}, are related and how.
The key reason is that the knowledge acquisition methods alluded to know of {\em A} and {\em B} only if they have occurred
in an explicit way and in close proximity in a sentence. Indeed, we
know of no successful application of the large scale existential
knowledge acquisition efforts to textual inference.
  
We focus here on relations between {\em any} two concepts, but only basic relations such as {\em ancestor} and {\em sibling}. This is motivated by work on Textual Inference~\cite{DaganGlMa06,HaghighiNMa05,BGPRS05}, where it has been
argued quite convincingly, (e.g.~\cite{maccartney-manning:2008:PAPERS}), that many
inferences are largely compositional and depend on the ability of 
models to recognize these basic relations between entities, noun phrases, etc. For example, it is often necessary to
know of an {\em ancestor} relation and its directionality in order to
deduce that a statement with respect to the {\em child} (e.g., {\em
  cannabis}) holds for an {\em ancestor} (e.g.,{\em drugs}). This is
illustrated in the following example, taken from the a test set of a
textual entailment challenge:

{\small
  \begin{quote}

    {\bf T}: Nigeria's National Drug Law Enforcement Agency (NDLEA)
    has seized 80 metric tonnes of {\em cannabis} in one of its
    largest ever hauls, officials say.

    {\bf H}: Nigeria seizes 80 tonnes of {\em drugs}.
  \end{quote}
}

Similarly, it is often important to know of a {\em sibling} relation
to infer that a statement about {\em Taiwan} may {\em contradict} an
identical statement with respect to {\em Japan} (at least, without
additional information) since these are {\em different}
countries.

\ignore{This is illustrated in the following example from RTE4 test
  data:

{\small
  \begin{quote}

    {\bf T}: A strong earthquake struck off the southern tip of {\em
      Taiwan} at 12:26 UTC, triggering a warning from Japan's
    Meteorological Agency that a 3.3 foot tsunami could be heading
    towards Basco, in the Philippines.

    {\bf H}: An earthquake strikes {\em Japan}.
  \end{quote}
} }

This paper proposes to address the problem of relation identification
and classification in a form that is directly applicable to textual
inference.  \ignore{Specifically, our system accepts two input
  concepts as arguments (these could be entities or noun phrases) and
  identifies the relation between them \ignore{along with its possible
    label}. For example, we identify that {\em global warming} and
  {\em food crisis} are in a {\em sibling} relation, and the concept
  of {\em economic problems} is in an {\em ancestor} relation with
  both of them.}  We focus here on the {\em ancestor} relation and the
{\em sibling} relation that were identified as key relations also in
\cite{maccartney-manning:2008:PAPERS} (they call a {\em sibling}
relation an {\em alternation}, and our {\em ancestor} relation {\em
  forward entailment} and {\em backward entailment}). Following their
argument, we expect that the resource developed in this work can be
used compositionally to support robust textual inference.

\ignore{ Our approach can discover whether two input entities pose an
  alternation (or non-exhaustive exclusion) relation ({\em red} $|$
  {\em green}), forward entailment ({\em Mel Gibson} $\sqsubset$ {\em
    actor}), reverse entailment ({\em flower} $\sqsupset$ {\em lily}),
  or independence ({\em Boeing 747} $\#$ {\em Valentine}). }

We develop an approach that makes use of Wikipedia and that, given a
pair of concepts will, on the fly, recognize and classify the relation
between these concepts. Because Wikipedia is a vast, rapidly-changing
resource, our approach needs to take into account its noisiness and
non-uniformity. Our algorithmic approach therefore treats Wikipedia
and its category structure as an open resource and uses statistical
text mining techniques to gather robust information.
%
\ignore{For example, the concept {\em Ford} appears many times in
  Wikipedia and is part of a large number of categories. As a {\em
    president}, mentions of {\em Ford} are consistent with mentions of
  other presidents. However, {\em Ford} also appears in other senses,
  for example, related to the {\em car} industry.  We need to
  disambiguate it, determine which category it belongs to and,
  within this category, which specific concept is intended. In order
  to disambiguate it, we make use of the context provided by the
  concept pair---{\em Ford} in ({\em Ford}, {\em Nixon}) is probably
  different than the one in ({\em Ford}, {\em Chevrolet}) as well as
  the one in ({\em Ford}, {\em Iacocca}).}
%
\ignore{Textual inference is driven by background
  knowledge. Therefore, the notion of {\em prominence} is
  essential. Most people know that {\em Michael Jordan} is a former
  NBA player, but they most likely do not know the {\em Michael
    Jordan} who attends school with the authors.  Consequently, unless
  additional knowledge is given, a textual inference system should
  assume that {\em Michael Jordan} is a basketball player. This is why
  we use Wikipedia as our background knowledge source. Moreover, under
  the assumption that in textual inference applications we are in
  search of some notion of ``common sense'' knowledge, we make use of
  a notion of prominence with respect to a given text collection (in
  this case, with respect to Wikipedia itself).}
  %
While Wikipedia has broad coverage, there is a need to go beyond it.
We suggest a simple but efficient technique to accomplish this using
web search, and show its effectiveness when at least one of the target
concepts is not mentioned in Wikipedia.
%
Our key technical contribution is a novel approach that makes use of
constraint-based inference with relational constraints to accurately
identify relations; we make use of our machine learning approach
multiple times, on automatically generated network of concepts that
are related to the target pair, and use constrained optimization
techniques to force these decisions to be coherent, thus improving the
accuracy of the decision.

\ignore{We measure the performance of our system over a large number
  of pairs chosen from over 40 semantic classes. We compare it with
  other large scale efforts to identify relations between
  concepts. For example we show that, even when all concepts are
  covered by the {\em extended WordNet}~\cite{Snow2006}, our system
  still significantly outperforms that system. }
%
%We also show examples indicating the contribution of our
%relation identification to textual inference.

The key contributions of this paper are (i) the definition of a
relation identification problem so that it is directly relevant to
supporting textual inference, (ii) the development of a robust and
accurate machine learning-based approach to address this problem, and
(iii) a novel approach that incorporates
prior knowledge within a constraint-based inference model to
accurately identify concept relations. We show that our system performs
significantly better than other systems built on existing large-scale
resources in identifying relational knowledge.

\ignore{Notably, our algorithmic approach is trained with a small
  number of annotated examples and generalizes well across semantic
  classes.}

\ignore{The rest of this paper is organized as follows. In
  Section~\ref{sec:relatedwork}, we briefly mention about previous
  work that inspired our approach.  Section~\ref{sec:approach}
  formalizes the problem and describes our algorithmic approach to
  relation detection and classification. Our experiments and results
  are described in Section \ref{sec:experiments}. Discussion and
  future work are in Section \ref{sec:discussion}.}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "jupiter"
%%% End: 
