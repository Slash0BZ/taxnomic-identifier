Our approach is a machine learning based algorithm. We first present
an algorithm to disambiguate given concepts. We then extract features
of disambiguated concepts and train a supervised multi-class
classifier to identify relations between concepts using Wikipedia as
background knowledge. Given a pair of concepts ($X$, $Y$), we focus on
addressing the following relations:

\begin{enumerate}
\item $X$ is an ancestor of $Y$, denoted by $X \leftarrow Y$.
\item $Y$ is an ancestor of $X$, denoted by $X \rightarrow Y$.
\item $X$ and $Y$ are siblings, denoted by $X \leftrightarrow Y$.
\item $X$ and $Y$ have no relation, denoted by $X \nleftrightarrow Y$.
\end{enumerate}

We also propose a simple but effective method to improve the coverage
of our system by going beyond Wikipedia to identify relations between
concepts that are not mentioned in Wikipedia.

\subsection{Concept Disambiguation}
\label{sec:conc-disamb}

\ignore{In this paper, we address the problem of identifying relation between
concepts.
Our problem is different than other previous work on relation
extraction. Most of traditional relation extraction tasks often focus
on extracting all possible entity pairs that follow some pre-defined
relations (e.g. LocatedIn, BornInYear, AuthorOf, etc.) in big corpora
\cite{banko-etzioni:2008:ACLMain}. Some other previous work extracting
all possible entity pairs and relations from big corpora with a list
of same class entities as the start point
\cite{davidov-rappoport:2008:ACLMain2}. Another direction of relation
extraction is to extract all possible instances of a given class, or
in the same class of given instances
\cite{WangCohen09,kozareva-riloff-hovy:2008:ACLMain}.}

\ignore{In this paper, we limit the relations of interest to \emph{Ancestor},
\emph{Sibling}, or \emph{None} relations.} By posing the problem as
identifying relations, we have to deal with the problem of
disambiguating given concepts. The more accurate the disambiguation
is, the more precise the relation recognition. For example, the
concept \emph{ford} in the concept pair \emph{(bush, ford)} may refer
to several concepts, such as \emph{Ford Motor Company}, president
\emph{Gerald Ford}, or \emph{Ford, Wisconsin - USA}. Our
approach is motivated by observing that the concept \emph{bush} can be
used to disambiguate \emph{ford} and vice versa. Ultimately, we can
place the two concepts \emph{George W. Bush} and \emph{Gerald Ford} in the
top retrieved lists of \emph{bush} and \emph{ford}.

We use Explicit Semantic Analysis (ESA) \cite{GabrilovichMa07} to
generate a list of top semantically relevant articles in Wikipedia for both given
concepts. For each article in the top list, we extract and keep its
categories. All categories are then tokenized to create a bag of
tokens relevant to the two given concepts.  We use the TF-IDF score to
weigh the importance of each token. Only the top tokens, which
are different from the two input entities, are kept for use in the next
step of our algorithm. 
% The general idea is that only the important tokens which are relevant
% to the two input entities are used to provide more information for the
% input entities so that we can determine the correct concepts to which
% they refer.
Each given concept is then concatenated with the top weighed tokens
to create a new query. We use the new queries to search titles and
texts of Wikipedia articles to get the top relevant articles. These
lists of articles are later used to provide features for our relation
classifier.

Figure \ref{fig:entity-disamb} shows the pseudo codes of our concept
disambiguation algorithm. In this algorithm, function $ESA(query)$
retrieves the most semantically relevant articles in Wikipedia to
$query$. All categories of a $list$ of articles are extracted by
function $getCategories(list)$. Function $search(query)$ retrieves
relevant articles in Wikipedia by searching $query$ in the articles'
titles and texts. 

\begin{figure}[!hbt]
  \begin{centering}
    {\scriptsize
      \fbox{
        \begin{minipage}{6in} 
          \begin{tabbing}
            {\textsc{Concept Disambiguation Algorithm}} \\
            \qquad {\textsc{Input}}: Two concepts $X$ and $Y$, and number of top tokens: N \\
            \qquad {\textsc{Output}}: A list of relevant articles in Wikipedia \\
            \qquad \qquad \qquad ~~for each concept. \\
            \\
            \qquad Query $q \leftarrow $ Concatenating $X$ and $Y$; \\
            \qquad $\mathcal{L} = \text{ESA}(q)$; \\ %// return a list of relevant articles to $X$ and $Y$ \\
            \\
            \qquad $\mathcal{C} = \text{getCategories}(\mathcal{L})$; \\
            \qquad $\mathcal{T} = {\text{tokenize}(\mathcal{C})}$; \\
            \qquad $\mathcal{T}_{top} = \text{top}_{tfidf}(\mathcal{T}, N)$; \\ %// pick top $N$ tokens\\
            \\
            \qquad Query $q_X \leftarrow $ Concatenating $X$ and tokens from $\mathcal{T}_{top}$; \\
            \qquad Query $q_Y \leftarrow $ Concatenating $Y$ and tokens from $\mathcal{T}_{top}$; \\
            \qquad $\mathcal{A}_X = \text{search}(q_X)$; \\
            \qquad $\mathcal{A}_Y = \text{search}(q_Y)$; \\
            \\
            \qquad \textsc{Return}: $\mathcal{A}_X$ and $\mathcal{A}_Y$; \\
          \end{tabbing}
        \end{minipage}
      }
  }
\end{centering}
\caption{Pseudo code of the concept disambiguation algorithm. The
  algorithm takes two concepts as input, disambiguates them and return
  a list of relevant Wikipedia articles for each.}
\label{fig:entity-disamb}
\end{figure}

\subsection{Learning Concept Relations}
\label{sec:learner}

Our goal is to design a supervised learning approach that identifies
relational knowledge between concepts. Our learning algorithm must be
as general as possible: Even if it is trained on examples with
concepts in certain semantic classes, it will still be able to
accurately classify examples in other semantic classes. To do this, we
first select expressive features to be associated with given
concept pairs. These features are necessarily independent of the
semantic class of the given concepts. Recall that, after the concept
disambiguation step, each concept of a given pair is associated with a
list of relevant articles in Wikipedia. These articles give lists of
titles, texts, and categories for the corresponding concepts. From now
on, we use {\em the titles of concept $X$} to refer to the titles of
the articles associated with concept $X$; similarly for {\em the texts
  of concept $X$}, and {\em the categories of concept $X$}. As
a learning algorithm, we use a regularized averaged Perceptron
\cite{FreundSc99}. The features selected for our learning problem
include the word usage of the associated articles, the association
information between two concepts, and the overlap ratios of important
information between the concepts and the articles. We describe these
features below.

\subsubsection{Bag of Words}

One of the most important features in recognizing relations between
concepts is the word usage in associated articles. An article in
Wikipedia typically contains three main components: title, text, and
categories. The title distinguishes a concept from other concepts in
Wikipedia. The text describes the concept. The categories classify the
concept into one or more concept groups which can be further
categorized. To collect the categories for a concept, we take the
categories of its associated articles and go up to $K$ level in the
Wikipedia category system. In our experiments, we use abstracts of
Wikipedia articles instead of whole texts.

By analyzing Wikipedia articles, we observe that the word usage
of the texts describing a concept often overlaps with the word usage
of the categories of its inferior concepts. For example, the texts
describing the concept \emph{Presidents of the United States} overlap
with the categories of \emph{George W. Bush}, and \emph{Gerald Ford}
in the tokens \emph{president, united, states} and so on (see Table
\ref{tab:snippets}.)

\begin{table*}[ht]
  \small
  \centering
  \begin{tabular}{|p{1.0in}|p{3.5in}|p{2.0in}|}
    \hline
    {\bf Concept} & {\bf Text} & {\bf Categories} \\
    \hline
    \hline
    President of the United States & \textit{The President of the United States is the head of state and head of government of the United States and is the highest political official in the United States by influence and recognition. The President leads the executive branch of the federal government and is one of only two elected members of the executive branch...} & \textit{Presidents of the United States, Presidency of the United States} \\
    \hline
    George W. Bush & \textit{George Walker Bush; born July 6, 1946) served as the 43rd President of the United States from 2001 to 2009. He was the 46th Governor of Texas from 1995 to 2000 before being sworn in as President on January 20, 2001...} & \textit{Children of Presidents of the United States, Governors of Texas, Presidents of the United States, Texas Republicans...} \\
    \hline
    Gerald Ford & \textit{Gerald Rudolff Ford (born Leslie Lynch King, Jr.) (July 14, 1913 â€“ December 26, 2006) was the 38th President of the United States, serving from 1974 to 1977, and the 40th Vice President of the United States serving from 1973 to 1974.} & \textit{Presidents of the United States, Vice Presidents of the United States, Republican Party (United States) presidential nominees...} \\
    \hline
  \end{tabular}
  \caption{Examples of texts and categories of the concepts from Wikipedia: {\em President of the United States}, {\em George W. Bush} and {\em Gerald Ford}.}
  \label{tab:snippets}
\end{table*}

On the other hand, two sibling concepts often have overlap not only in
their texts, but also in their categories. For example, concepts
\emph{George W. Bush} and \emph{Gerald Ford} both use
\emph{presidents, united, states} in their texts, as well as
\emph{presidents, united, states, republican} in their categories.

%We refer to the similarity in word usage between the texts of entity
%$x$ and the categories of entity $y$ as $sim(T_x, C_y)$. Similarly, we
%have $sim(C_x, T_y)$ as the similarity score between the categories of
%$x$ and the texts of $y$. We also use $sim(T_x, T_y)$ and $sim(C_x,
%C_y)$ to refer to the similarity scores of the texts and categories,
%respectively, of the two input entities $x$, and $y$.

We define four word usage features associated with any two given
concepts $X$ and $Y$: the degree of similarity in word usage between
the texts of concept $X$ and the categories of concept $Y$, the
similarity between the categories of $X$ and the texts of $Y$, the
similarity between the text of $X$ and the text of $Y$, and the
similarity in word usage between the categories of $X$ and the
categories of $Y$.

We must now measure the degree of similarity in word usage for the
features. There are several ways to calculate the degree of similarity
between two texts \cite{mohler-mihalcea:2009:EACL}. In this paper, we
use the cosine similarity. Equation (\ref{eq:1}) shows the formula
of the cosine similarity metric applied to the term vectors $T_1$ and
$T_2$ of two text fragments.

\begin{align}
\label{eq:1}
     Sim(T_1, T_2) & = cos \theta(T_1, T_2) = \frac{T_1 \cdot T_2}{\left\| T_1 \right\| \left\| T_2 \right\|} \nonumber \\ 
                   & = \frac{\sum_{i=1}^{N}x_iy_i}{\sqrt{\sum_{j=1}^{N}x_j} \sqrt{\sum_{k=1}^{N}y_k}}  
\end{align}

% \begin{equation}
% \label{eq:1}
%      Sim(X, Y) = cos \theta(X, Y) = \frac{X \cdot Y}{\left\| X \right\| \left\| Y \right\|} = \frac{\sum_{i=1}^{N}x_iy_i}{\sqrt{\sum_{j=1}^{N}x_j} \sqrt{\sum_{k=1}^{N}y_k}}  
% \end{equation}

where $N$ is the vocabulary size, and $x_i$, $y_i$ are two indicator
values in $T_1$ and $T_2$, respectively.

\subsubsection{Association Information}
Another useful feature that can help in recognizing the relation between
two concepts is the association information between them.
% This feature is most beneficial for recognizing ancestor
% relation. It is because a superior is often mentioned about whenever
% we want to give a definition for a inferior. This is fit very well
% with the nature of Wikipedia articles: writing about things.
We capture the association information between two concepts $X$ and
$Y$ by measuring the pointwise mutual information, defined in equation
(\ref{eq:2}). In this paper, the association information is measured
at the document level.

\begin{align}
\label{eq:2}
  PMI(X, Y) & = log \frac{p(X, Y)}{p(X)p(Y)} = log \frac{\frac{f(X,Y)}{N}}{\frac{f(X)}{N} \frac{f(Y)}{N}} \nonumber \\ 
            & = log\frac{N f(X,Y)}{f(X)f(Y)}
\end{align}

% \begin{equation}
% \label{eq:2}
%   PMI(x, y) = log \frac{p(x, y)}{p(x)p(y)} = log \frac{\frac{C(x,y)}{N}}{\frac{C(x)}{N}\frac{C(y)}{N}} = log\frac{N \times C(x,y)}{C(x)C(y)}
% \end{equation}

where $X$ and $Y$ are two input entities, $N$ is the total number of
documents in Wikipedia, and $f(.)$ is a function returning document
frequency.

\subsubsection{Overlap Ratio}
Wikipedia articles are not only helpful for definitions and
explanations about concepts, they are also useful for categorizing
concepts into groups and topics. We want to capture the fact that the
titles of a concept often has an overlap with the categories of its
descendants. For examples, the title (and also the concept itself)
{\em Presidents of the United States} overlap with one of the
categories of the concepts {\em George W. Bush} and {\em Gerald Ford}
(see Table \ref{tab:snippets}.) This phenomenon is thus an optimal
feature for recognizing the ancestor relation. We measure this overlap
as the ratio of {\em common phrases} used in the titles of concept $X$
and the categories of concept $Y$. In our context, a phrase is
considered to be a {\em common phrase} if it appears in the titles of
$X$ and the categories of $Y$ and is one of the following: (1) the
{\em whole string} of a category of $Y$, or (2) the {\em head} in its
root form of a category of $Y$, or (3) the {\em post-modifier} of a
category of $Y$.

%\begin{itemize}
%\item the {\em whole string} of a category of $Y$, or
%\item the {\em head} in its root form of a category of $Y$, or
%\item the {\em post-modifier} of a category of $Y$.
%\end{itemize}

In our work, we use the Noun Group Parser from \cite{suchanek2007WWW}
to extract the {\em head} and {\em post-modifer} from a category. For
example, one of the categories of an article about \emph{Chicago} is
\emph{Cities in Illinois}. This category can be parsed into a head in
its root form \emph{City}, and a post-modifer \emph{Illinois}. Given
two entity pairs \emph{(Illinois, Chicago)} and \emph{(City,
  Chicago)}, we can recognize that \emph{Illinois} and \emph{City}
match the category {\em Cities in Illinois} of concept {\em
  Chicago}. Therefore, we have a strong indication that {\em Chicago}
is a descendant of both {\em Illinois} and {\em City}.

To collect the categories for a concept, we collect the categories of
its associated articles up to $K$ levels in the Wikipedia category
system. We measure the overlap ratio between the titles of concept $X$
and the categories of concept $Y$ by using the Jaccard similarity
coefficient as shown in equation (\ref{eq:3}).

\begin{equation}
  \label{eq:3}
  \sigma_{ttl}^K(X,Y) = \frac{\left| L_X \cap \mathcal{P}_Y^K \right|}{\left| L_X \cup \mathcal{P}_Y^K \right|}
\end{equation}

where $L_x$ is the set of the titles of $X$, and $\mathcal{P}_y$ is
the union of the category strings, the heads of the categories, and
the post-modifiers of the categories of $Y$.

Similarly, we also use the ratio $\sigma_{ttl}^K(Y,X)$ as one of our features.

% where $\mathcal{P}_y = \{C_y\} \cup \{H_y\} \cup \{D_y\}$, and $L_x$,
% $C_y$, $H_y$, $D_y$ are the titles of $x$, the category strings, the
% category's heads, and the category's post-modifiers of $y$,
% respectively.

We also observe many cases where two concepts are siblings or have no
relation but still have an overlap between their titles and
categories. To handle this, we add one more feature: the overlap ratio
of {\em common phrases} between the categories of two given
entities. However, to capture the sibling relation, we do not use the
{\em post-modifier} of the categories, because it does not help us
recognizing sibling relations (e.g. {\em Cities in Illinois} and {\em
  Mountains In Illinois} have the same {\em post-modifier}, which is
{\em Illinois}, but a city and a mountain cannot be siblings.)

The Jaccard similarity coefficient for overlap ratio between the
categories of two concepts $X$ and $Y$ is shown in equation (\ref{eq:4}).

\begin{equation}
  \label{eq:4}
  \sigma_{cat}^K(X,Y) = \frac{\left| \mathcal{Q}_X^K \cap \mathcal{Q}_Y^K \right|}{\left| \mathcal{Q}_X^K \cup \mathcal{Q}_Y^K \right|}
\end{equation}

where $\mathcal{Q}_{(.)}^K$ is the union of the category strings and
the heads of the categories. The categories are collected by going up
to $K$ level in the Wikipedia category system.

All of the features described above are used in training our
multi-class classifier to recognize relations between concepts.

% We use the features described above to train a multi-class classifier
% to recognize four relation types of relation for concept pairs.

\subsection{Going Beyond Wikipedia}
\label{sec:impr-syst-cover}
In our work, Wikipedia is the main source of background knowledge used
to recognize concept relations. Although most commonly used concepts
can be found in Wikipedia, there is still a need to identify relations
between other concepts that do not have relevant articles in
Wikipedia. We call these concepts {\em non-Wikipedia} concepts.  In
this section, we show how to improve the coverage of our approach to
concepts outside Wikipedia. For each such concept, we find a
replacement that is in Wikipedia. Our method was motivated by
\cite{1321585}. In their work, they address the set expansion problem.
In order to identify concept pairs that belong to lists, they look for
structures like {\em ``... ne$_a$, ne$_b$ and ne$_c$ ...'' } where
$ne_a$, $ne_b$, etc. are named entities. E.g. {\em ``I've lived in NY,
  Paris, and Amsterdam.''} Another possibility would be {\em
  ``... ne$_a$, ne$_b$ or ne$_c$ ...'' }. However, in their work, the
authors use texts from Wikipedia to look for entities having desired
structures. We are trying to cover concepts beyond
Wikipedia. Therefore, we use a modified version of their structures
along with Web search engines to search for concepts in textual
lists. In our work, we use the Yahoo! Web Search
APIs\footnote{http://developer.yahoo.com/search/web/} to search for
the conjunction of two given concepts (e.g. {\em ``bass''} AND {\em
  ``trout''} and look for the list structures {\em ``... $\left <
    DELIMITER \right >$ c$_a$ $\left < DELIMITER \right >$ c$_b$
  $\left < DELIMITER \right >$ c$_c$ $\left < DELIMITER \right >$
  ...''} in the top snippets returned. The delimiters used in our
experiments are comma(,), puntuation(.), and asterisk(*). For
sentences that contain our structures, we extract $c_a$, $c_b$,
etc. as candidate concepts which are in the same semantic class with
our original given concepts. To reduce noise from concepts extracted
from the snippets, we constrain the list structure to contain at least
4 concepts that are no longer than 20 characters each. Once
a list of concept candidates is extracted, the concepts are ranked
based on their frequency of occurrence. The highest ranked concepts
are used to replace the non-Wikipedia concepts in the original given
pair. The highest ranked candidates are tried with the concept
disambiguation algorithm until we find the concepts mentioned in
Wikipedia.

Note that unlike other pattern-based methods discussed in
sec.~\ref{sec:previous-work} that search for concepts in close
proximity, we use concepts in Wikipedia as anchors for the
search, thus increasing the likelihood of covering less
commonly used concepts.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "jupiter"
%%% End: 
