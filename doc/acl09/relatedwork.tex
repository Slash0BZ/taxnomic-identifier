\cite{ilprints665} constructs a hypernym-only classifier using the dependency path patterns discovered in the sentences that contain noun pairs in hypernym/hyponym relation.
Furthermore, they show that using coordinate terms can help improve the recall of the hypernym classifier.
The best system in their work is a hypernym-only classifier additionally trained with hundred thousands of dependency paths extracted from Wikipedia corpus.
The system outperforms the best WordNet classifier in identifying coordinated terms by relatively improving over 54\% in F-score.

In the other hand, \cite{citeulike:2915130} addresses the challenge of dealing with very large amount of data by employing efficient randomized algorithms to quickly cluster the noun similarity lists. The system first collects all nouns and their features, and after that constructs feature vectors for these nouns. The system then hashes all nouns into a hash table using the Locality Sensitive Hash functions, which can put similar feature vectors close to each other in the hash table. By using this approach, they can discover thousands of distinct clusters of English nouns by exploring over 70 million webpages. Recently, \cite{Snow2006} apply this algorithm and also the data of noun similarity lists to train their ($m$,$n$)-cousin relationship classifier. They combine their hypernym-only classifier and the ($m$,$n$)-cousin classifier with the evidence features to recognized the hypernym and cousin relationships among nouns. Their inferred taxonomy achieves the best performance after adding 30,000 novel hyponyms compared to those in WordNet-2.1. They show that their system relatively improves 23\% in F-score over the WordNet-2.1 hypernym classifier.

Along this research direction, \cite{citeulike:1587018,pacsca-vandurme:2008:ACLMain} proposed approaches to automatically acquire open-domain classes of entities and attributes by using very little supervised seed information. Web documents and query logs are used in the acquisition process. By applying their method, they are able to extract 4,583 classes associated with an average of 189 instances for each class. However, each instance has one parent and no further ancestor. There is no hierarchical structure among classes and instances.

All of these work utilizes either supervised data to train their classifier or a very large amount of data in web-scale to mine the interested facts. On one hand, supervised approaches requires a lot of well labeled data to train a good classifier. It is infeasible to use a large amount of available unannotated text to support the classification task. On the other hand, approaches exploring very large amounts of data may potentially have problem because of their inflexibility when one wants to add newly appearing documents into the corpus and re-generate the facts. We address these issues by using Wikipedia and propose an efficient light-weight approach using the Wikipedia category structure to identify the relations between entities. 